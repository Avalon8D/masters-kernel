{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T05:17:07.584608Z",
     "start_time": "2019-03-25T05:16:40.527061Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle, gzip, numpy, urllib.request, json\n",
    "\n",
    "# Load the dataset\n",
    "urllib.request.urlretrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T05:17:25.794086Z",
     "start_time": "2019-03-25T05:17:23.337588Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T06:08:29.076399Z",
     "start_time": "2019-03-25T06:08:28.879325Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame ()\n",
    "\n",
    "for label, (features, labels) in zip (\n",
    "    ('train_set', 'valid_set', 'test_set'),\n",
    "    (train_set, valid_set, test_set),\n",
    "):\n",
    "    df = df.append (pd.DataFrame (\n",
    "        {'features': feature, 'data': label, 'labels': a_label}\n",
    "        for feature, a_label in zip (features[:], labels)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T06:15:41.173863Z",
     "start_time": "2019-03-25T06:15:35.809208Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_parquet ('data/mnist_labeled.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def \n",
    "\n",
    "df.apply (f, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T23:09:45.133546Z",
     "start_time": "2019-03-09T23:09:45.131126Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist_image_dim = numpy.int64 (numpy.sqrt (train_set[0][:].shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T23:10:31.958360Z",
     "start_time": "2019-03-09T23:10:31.954296Z"
    }
   },
   "outputs": [],
   "source": [
    "digits, labels = train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generate bernoulli columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T23:09:54.251976Z",
     "start_time": "2019-03-09T23:09:52.288394Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import convolve, zoom\n",
    "sqrt, log = np.sqrt, np.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T23:09:55.648178Z",
     "start_time": "2019-03-09T23:09:55.642518Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def LIL_estimate (count, delta=1 / 64):\n",
    "    N = 1\n",
    "    delta_curr = np.float64 (delta)\n",
    "    curr_frac = np.float64 (.5)\n",
    "\n",
    "    while (count * curr_frac + sqrt (\n",
    "        2 * curr_frac * (1 - curr_frac) * count * log (log (count))\n",
    "    )) > (- 192 * log (delta_curr)):\n",
    "        N += 1\n",
    "        delta_curr /= 3.\n",
    "        curr_frac *= .5\n",
    "    \n",
    "    return N, delta_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T23:10:53.470828Z",
     "start_time": "2019-03-09T23:10:53.450449Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 0.00041152263374485596)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_len = len (digits)\n",
    "delta = .1\n",
    "\n",
    "N_max, deep_delta = LIL_estimate (digits_len, delta)\n",
    "N_max, deep_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T23:10:55.557403Z",
     "start_time": "2019-03-09T23:10:55.554639Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rand = np.random.rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T23:10:56.026155Z",
     "start_time": "2019-03-09T23:10:56.016156Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "zoom_factor = 3.0\n",
    "\n",
    "window_size = 12\n",
    "new_image_dim = int (mnist_image_dim * zoom_factor) // window_size\n",
    "\n",
    "def sample_normalize_cols (id_vec):\n",
    "    a_id, vec = id_vec\n",
    "    \n",
    "    new_vec = zoom (vec.reshape (\n",
    "        mnist_image_dim, mnist_image_dim\n",
    "    ), 3.0).reshape (\n",
    "        new_image_dim, window_size, new_image_dim, window_size\n",
    "    ).sum (axis=3).sum (axis=1).reshape (-1)\n",
    "    \n",
    "    new_vec /= window_size * window_size\n",
    "    new_vec /= np.linalg.norm (new_vec)\n",
    "    \n",
    "    return (\n",
    "        (rand (N_max) < .5).cumprod ().astype (np.bool), \n",
    "        (a_id, new_vec)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:44:42.610705Z",
     "start_time": "2019-03-06T16:44:42.561308Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rdd_subsampled = rdd_digits.map (sample_normalize_cols).persist ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:44:46.989586Z",
     "start_time": "2019-03-06T16:44:42.614137Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_subsampled.take (1)[0][-1][-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop to sample with ridge leverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:44:47.215054Z",
     "start_time": "2019-03-06T16:44:46.992705Z"
    }
   },
   "outputs": [],
   "source": [
    "from gc import collect\n",
    "\n",
    "from scipy.linalg import pinvh, eigh, cholesky\n",
    "\n",
    "import numba\n",
    "import numba.types as ntypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kernel func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:44:48.105868Z",
     "start_time": "2019-03-06T16:44:47.217761Z"
    }
   },
   "outputs": [],
   "source": [
    "@numba.jit (\n",
    "    ntypes.float64 (ntypes.float64[:], ntypes.float64[:]),\n",
    "    nopython=True, nogil=True\n",
    ")\n",
    "def kernel_func (x, y):\n",
    "    \n",
    "    return x.dot (y)\n",
    "\n",
    "@numba.jit (\n",
    "    ntypes.float64[:, :] (\n",
    "        ntypes.float64[:, :], ntypes.float64, \n",
    "        ntypes.float64[:, :], ntypes.float64[:]\n",
    "    ),\n",
    "    looplift=True, nogil=True, nopython=True\n",
    ")\n",
    "def fast_kernel_gram (X, a_lamb, out, iknorms_X):    \n",
    "    for i in range (X.shape[0]):\n",
    "        out[i, i] = kernel_func (X[i], X[i])\n",
    "        iknorms_X[i] = 1 / np.sqrt (out[i, i])\n",
    "        out[i, i] += a_lamb\n",
    "        \n",
    "        for j in range (i):\n",
    "            n_kernel_ij = kernel_func (X[i], X[j]) * iknorms_X[i] * iknorms_X[j]\n",
    "            out[i, j] = n_kernel_ij\n",
    "            out[j, i] = n_kernel_ij\n",
    "    \n",
    "    return out\n",
    "\n",
    "def kernel_gram (X, a_lamb=0.0):\n",
    "    out = np.empty ((X.shape[0], X.shape[0]))\n",
    "    iknorms_X = np.empty (X.shape[0])\n",
    "\n",
    "    return fast_kernel_gram (X, a_lamb, out, iknorms_X)\n",
    "\n",
    "@numba.jit (\n",
    "    ntypes.float64[:, :] (\n",
    "        ntypes.float64[:, :], ntypes.float64[:, :], \n",
    "        ntypes.float64[:, :], ntypes.float64[:]\n",
    "    ),\n",
    "    looplift=True, nogil=True, nopython=True\n",
    ")\n",
    "def fast_kernel_cross (X, Y, out, iknorms_Y):\n",
    "    for i in range (Y.shape[0]):\n",
    "        iknorms_Y[i] = 1 / np.sqrt (kernel_func (Y[i], Y[i]))\n",
    "    \n",
    "    for i in range (X.shape[0]):\n",
    "        iknorm_Xi = 1 / np.sqrt (kernel_func (X[i], X[i]))\n",
    "        \n",
    "        for j in range (Y.shape[0]):\n",
    "            out[i, j] = kernel_func (X[i], Y[j]) * iknorm_Xi * iknorms_Y[j]\n",
    "    \n",
    "    return out\n",
    "\n",
    "def kernel_cross (X, Y):\n",
    "    out = np.empty ((X.shape[0], Y.shape[0]))\n",
    "    iknorms_Y = np.empty (Y.shape[0])\n",
    "    \n",
    "    return fast_kernel_cross (X, Y, out, iknorms_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:44:49.936888Z",
     "start_time": "2019-03-06T16:44:48.108693Z"
    }
   },
   "outputs": [],
   "source": [
    "@numba.jit (\n",
    "    ntypes.float64 (ntypes.float64[:], ntypes.float64[:], ntypes.float64[:, :]),\n",
    "    nopython=True, nogil=True\n",
    ")\n",
    "def leverage_scores (x, Kx, isqrt_G):\n",
    "    x_Kproj = isqrt_G.dot (Kx)    \n",
    "\n",
    "    return kernel_func (x, x) - x_Kproj.dot (x_Kproj)\n",
    "\n",
    "@numba.jit (\n",
    "    ntypes.float64[:] (\n",
    "        ntypes.float64[:, :], ntypes.float64[:, :], \n",
    "        ntypes.float64[:, :], ntypes.float64[:]\n",
    "    ),\n",
    "    looplift=True, nogil=True, nopython=True\n",
    ")\n",
    "def fast_mat_leverage_score (X, KX, isqrt_G, out):\n",
    "    X_Kproj_T = isqrt_G.dot (KX.T)\n",
    "    X_Kproj_T *= X_Kproj_T\n",
    "    X_Kproj_dot = X_Kproj_T.sum (axis=0)\n",
    "    \n",
    "    for i in range (X.shape[0]):\n",
    "        out[i] = kernel_func (X[i], X[i]) - X_Kproj_dot[i]\n",
    "\n",
    "    return out\n",
    "\n",
    "def mat_leverage_score (X, KX, isqrt_G):\n",
    "    out = np.empty (X.shape[0])\n",
    "\n",
    "    return fast_mat_leverage_score (X, KX, isqrt_G, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Leverage Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:44:49.948729Z",
     "start_time": "2019-03-06T16:44:49.940056Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pair_leverages_sample (rows, base_sample):\n",
    "    rows_ids_values = list (rows)\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    rows_ids = np.fromiter (\n",
    "        (row[0] for row in rows_ids_values), \n",
    "        dtype=np.int64\n",
    "    )\n",
    "\n",
    "    rows_mat = np.vstack (\n",
    "        row[-1] for row in rows_ids_values\n",
    "    ).astype (np.float64)\n",
    "\n",
    "    del rows_ids_values\n",
    "\n",
    "    #  gram evaluation\n",
    "    sample_km = kernel_gram (base_sample, lamb)\n",
    "    isample_km = pinvh (sample_km); del sample_km\n",
    "\n",
    "    # cholesky opf inverse gram evaluation\n",
    "    km_isqrt = cholesky (isample_km, overwrite_a=True, check_finite=False)\n",
    "\n",
    "    # cross kernel evaluation\n",
    "    cross_km = kernel_cross (rows_mat, base_sample)\n",
    "\n",
    "    leverages = mat_leverage_score (rows_mat, cross_km, km_isqrt)\n",
    "    del cross_km, km_isqrt, isample_km\n",
    "\n",
    "    leverages *= 1.5 / lamb\n",
    "\n",
    "    collect ()\n",
    "\n",
    "    return (\n",
    "        (leverage, (a_id, row)) \n",
    "        for a_id, leverage, row \n",
    "        in zip (rows_ids, leverages, rows_mat[:])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## First Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:44:59.930330Z",
     "start_time": "2019-03-06T16:44:49.954443Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_sample_ids_values = rdd_subsampled.filter (lambda row: row[0][-1]).values ().collect ()\n",
    "\n",
    "base_sample = np.vstack (\n",
    "    value for _, value in base_sample_ids_values\n",
    ").astype (np.float64)\n",
    "\n",
    "base_sample_ids = np.fromiter (\n",
    "    (a_id for a_id, _ in base_sample_ids_values),\n",
    "    dtype=np.int64,\n",
    ")\n",
    "\n",
    "del base_sample_ids_values\n",
    "\n",
    "collect ()\n",
    "\n",
    "lamb = .01\n",
    "i = N_max - 2\n",
    "delta_curr = deep_delta * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Subsampled loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T18:01:40.519221Z",
     "start_time": "2019-03-06T16:44:59.934333Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806, 49)\n",
      "133.28688257387546 0.08087796272686616 11.58953806471177\n",
      "(1984, 49)\n",
      "95.04302748185847 0.03197948434786624 10.15275166892289\n",
      "(2880, 49)\n",
      "122.4805965425398 0.02051257687867021 9.30776229219804\n",
      "(4014, 49)\n",
      "165.38816222589227 0.013692206492747095 8.509492591109387\n",
      "(5517, 49)\n",
      "236.75244828948806 0.009658240455655697 7.76960016625064\n",
      "(7394, 49)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 18 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1838)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b2cb2f3814ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m ).persist ()\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mleverage_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd_leverages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mleverage_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd_leverages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \"\"\"\n\u001b[0;32m-> 1044\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \"\"\"\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 18 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1838)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "print (base_sample.shape)\n",
    "\n",
    "while i >= 0:\n",
    "    rdd_leverages = rdd_subsampled.filter (\n",
    "        lambda row: row[0][i]\n",
    "    ).values ().mapPartitions (\n",
    "        lambda rows: pair_leverages_sample (rows, base_sample), \n",
    "        preservesPartitioning=True\n",
    "    ).persist ()\n",
    "\n",
    "    leverage_sum = rdd_leverages.keys ().sum ()\n",
    "\n",
    "    leverage_mean = rdd_leverages.keys ().mean ()\n",
    "\n",
    "    log_leverage_sum = 1 * log (leverage_sum / delta_curr)\n",
    "\n",
    "    print (leverage_sum, leverage_mean, log_leverage_sum)\n",
    "\n",
    "    base_sample_ids_values = rdd_leverages.filter (lambda row:\n",
    "        rand () < row[0] * log_leverage_sum\n",
    "    ).values ().collect ()\n",
    "    \n",
    "    base_sample_ids = np.hstack ((\n",
    "        base_sample_ids,\n",
    "        *(a_id for a_id, _ in base_sample_ids_values)\n",
    "    ))\n",
    "    \n",
    "    base_sample = np.vstack ((\n",
    "        base_sample,\n",
    "        *(value for _, value in base_sample_ids_values)\n",
    "    )).astype (np.float64)\n",
    "    \n",
    "    del base_sample_ids_values\n",
    "    rdd_leverages.unpersist ();\n",
    "    \n",
    "    collect ()\n",
    "\n",
    "    i -= 1\n",
    "#     i = i if i > 0 else 0\n",
    "    delta_curr *= 3\n",
    "    \n",
    "    print (base_sample.shape)\n",
    "    \n",
    "rdd_leverages = rdd_subsampled.values ().mapPartitions (\n",
    "    lambda rows: pair_leverages_sample (rows, base_sample), \n",
    "    preservesPartitioning=True\n",
    ").persist ()\n",
    "\n",
    "leverage_sum = rdd_leverages.keys ().sum ()\n",
    "\n",
    "leverage_mean = rdd_leverages.keys ().mean ()\n",
    "\n",
    "log_leverage_sum = 1 * log (leverage_sum / delta_curr)\n",
    "\n",
    "print (leverage_sum, leverage_mean, log_leverage_sum)\n",
    "\n",
    "base_sample_ids_values = rdd_leverages.filter (lambda row:\n",
    "    rand () < row[0] * log_leverage_sum\n",
    ").values ().collect ()\n",
    "\n",
    "base_sample_ids = np.hstack ((\n",
    "    base_sample_ids,\n",
    "    *(a_id for a_id, _ in base_sample_ids_values)\n",
    "))\n",
    "\n",
    "base_sample = np.vstack ((\n",
    "    base_sample,\n",
    "    *(value for _, value in base_sample_ids_values)\n",
    ")).astype (np.float64)\n",
    "\n",
    "del base_sample_ids_values\n",
    "rdd_leverages.unpersist ();\n",
    "\n",
    "collect ()\n",
    "\n",
    "print (base_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Full loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T14:54:57.986643Z",
     "start_time": "2019-03-06T14:17:16.672939Z"
    },
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_sample_a = base_sample\n",
    "base_sample_a_ids = base_sample_ids\n",
    "\n",
    "print (base_sample_a.shape)\n",
    "\n",
    "for i in range (20):\n",
    "    rdd_leverages = rdd_subsampled.values ().mapPartitions (\n",
    "        lambda rows: pair_leverages_sample (rows, base_sample_a),\n",
    "        preservesPartitioning=True\n",
    "    ).persist ()\n",
    "\n",
    "    leverage_sum = rdd_leverages.keys ().sum ()\n",
    "\n",
    "    leverage_mean = rdd_leverages.keys ().mean ()\n",
    "\n",
    "    log_leverage_sum = 1 * log (leverage_sum / delta)\n",
    "\n",
    "    print (leverage_sum, leverage_mean, log_leverage_sum)\n",
    "\n",
    "    base_sample_a_ids_values = rdd_leverages.filter (lambda row:\n",
    "        rand () < row[0]\n",
    "    ).values ().collect ()\n",
    "    \n",
    "    base_sample_a = np.vstack ((\n",
    "        base_sample_a,\n",
    "        *(value for _, value in base_sample_a_ids_values)\n",
    "    )).astype (np.float64)\n",
    "\n",
    "    base_sample_a_ids = np.hstack ((\n",
    "        base_sample_a_ids,\n",
    "        *(a_id for a_id, _ in base_sample_a_ids_values)\n",
    "    ))\n",
    "    \n",
    "    del base_sample_a_ids_values\n",
    "    rdd_leverages.unpersist ();\n",
    "    \n",
    "    collect ()\n",
    "\n",
    "    i -= 1\n",
    "    i = i if i > 0 else 0\n",
    "    delta_curr *= 3\n",
    "    \n",
    "    print (base_sample_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Leverages Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T14:55:15.102793Z",
     "start_time": "2019-03-06T14:55:15.098606Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T14:55:16.725845Z",
     "start_time": "2019-03-06T14:55:16.641126Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rdd_leverages = rdd_subsampled.values ().mapPartitions (\n",
    "    lambda rows: pair_leverages_sample (rows, base_sample_a), \n",
    "    preservesPartitioning=True\n",
    ").persist ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T14:55:58.063112Z",
     "start_time": "2019-03-06T14:55:19.487494Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_leverages = rdd_leverages.keys ().map (\n",
    "    lambda val: Row (leverage_score=float (val))\n",
    ").toDF ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T15:22:19.518502Z",
     "start_time": "2019-03-06T14:55:58.066660Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_leverages.describe ().show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:32:28.579883Z",
     "start_time": "2019-03-06T16:32:28.253600Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_leverages.approxQuantile (\n",
    "    'leverage_score', \n",
    "    [\n",
    "        float (i) for i in np.linspace (0, 1, 11)\n",
    "    ], \n",
    "    .01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Class splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

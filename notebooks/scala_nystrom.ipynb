{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:51:43.242372Z",
     "start_time": "2019-04-22T16:51:06.746Z"
    }
   },
   "outputs": [],
   "source": [
    "%Truncation on\n",
    "%AddDeps org.scalanlp breeze_2.12 1.0-RC2 --transitive --trace --verbose\n",
    "%AddDeps org.scalanlp breeze-natives_2.12 1.0-RC2 --transitive --trace --verbose\n",
    "%AddDeps org.scalanlp breeze-viz_2.12 1.0-RC2 --transitive --trace --verbose\n",
    "%ShowTypes on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addapt to importing from jar :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:51:43.438956Z",
     "start_time": "2019-04-22T16:51:06.750Z"
    }
   },
   "outputs": [],
   "source": [
    "import kernel_lib._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:51:43.714468Z",
     "start_time": "2019-04-22T16:51:06.754Z"
    }
   },
   "outputs": [],
   "source": [
    "import com.github.fommil.netlib.LAPACK.{getInstance => lapack}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## kernel func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:51:45.179281Z",
     "start_time": "2019-04-22T16:51:06.758Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def kernel_func (\n",
    "    x:breeze.linalg.DenseVector[Double], \n",
    "    y:breeze.linalg.DenseVector[Double]\n",
    "):Double = x.t * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Spark Digits Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:51:45.450471Z",
     "start_time": "2019-04-22T16:51:06.762Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.{sql => s_sql}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:51:46.230876Z",
     "start_time": "2019-04-22T16:51:06.764Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val max_per_P = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:51:59.320520Z",
     "start_time": "2019-04-22T16:51:06.766Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val path = \"hdfs://thesis-tiny-python3-anaconda-m/user/pedro.schmidt/digits.snappy.parquet\"\n",
    "//val path = \"digits.snappy.parquet\"\n",
    "\n",
    "var df_digits_raw = spark.read.format (\n",
    "    \"parquet\"\n",
    ").load (path).withColumn (\n",
    "    \"features\", $\"features\".cast (\"ARRAY<DOUBLE>\")\n",
    ").withColumn (\n",
    "    \"id\", s_sql.functions.monotonically_increasing_id\n",
    ")\n",
    "\n",
    "df_digits_raw = df_digits_raw.repartition (df_digits_raw.count ().toInt / max_per_P).persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:52:00.327220Z",
     "start_time": "2019-04-22T16:51:06.768Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_digits_raw.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nystrom algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Features transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:52:12.980318Z",
     "start_time": "2019-04-22T16:51:06.770Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val image_dim = scala.math.sqrt (\n",
    "    df_digits_raw.select (\n",
    "        s_sql.functions.size ($\"features\")\n",
    "    ).map (_.getInt (0)).take (1)(0)\n",
    ").toInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:52:17.610224Z",
     "start_time": "2019-04-22T16:51:06.773Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// window is centered\n",
    "\n",
    "val window_x = 7\n",
    "val window_y = 7\n",
    "val skip_x = 3\n",
    "val skip_y = 3\n",
    "\n",
    "def max_pool (\n",
    "    I:breeze.linalg.DenseMatrix[Double]\n",
    "):breeze.linalg.DenseMatrix[Double] = {\n",
    "    // column major implies image will be\n",
    "    val I_pooled = new breeze.linalg.DenseMatrix[Double](\n",
    "        (I.rows - window_y) / skip_y, \n",
    "        (I.cols - window_x) / skip_x\n",
    "    )\n",
    "    \n",
    "    for (\n",
    "        (i, im, ip) <- (0 until I_pooled.rows).map (\n",
    "            (v:Int) => (v, skip_y * v, skip_y * v + window_y)\n",
    "        );\n",
    "        (j, jm, jp) <- (0 until I_pooled.cols).map (\n",
    "            (v:Int) => (v, skip_x * v, skip_x * v + window_x)\n",
    "        )\n",
    "    ) {\n",
    "        I_pooled (i, j) = breeze.linalg.max (I (im until ip, jm until jp))\n",
    "    }\n",
    "    \n",
    "    I_pooled\n",
    "}\n",
    "\n",
    "val max_pool_udf = s_sql.functions.udf (\n",
    "    (x:Seq[Double]) => {\n",
    "        max_pool (\n",
    "            new breeze.linalg.DenseMatrix (image_dim, image_dim, x.toArray)\n",
    "        ).data.toSeq\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:52:18.694642Z",
     "start_time": "2019-04-22T16:51:06.775Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val df_data = df_digits_raw.select (\n",
    "    LinalgUtils.normalize_udf (\n",
    "        $\"features\"\n",
    "//         max_pool_udf ($\"features\")\n",
    "    ).as (\"nfeatures\"), $\"label\", $\"id\"\n",
    ").persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:52:31.021698Z",
     "start_time": "2019-04-22T16:51:06.777Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val n = df_data.count ().toInt\n",
    "val rows = df_data.select (\n",
    "    s_sql.functions.size ($\"nfeatures\")\n",
    ").map (_.getInt (0)).take (1)(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Nystrom Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:49:35.356678Z",
     "start_time": "2019-04-10T18:49:11.795Z"
    },
    "hidden": true
   },
   "source": [
    "val kappa = 1.0\n",
    "val t = .3\n",
    "val base_lamb = kappa * kappa / scala.math.min (t, 1.0)\n",
    "val q = 1.5\n",
    "// target lambda is 1 / sqrt (n)\n",
    "val H = (scala.math.log (base_lamb * scala.math.sqrt (n)) / scala.math.log (q)).toInt\n",
    "val delta = .9\n",
    "\n",
    "val q2 = 54 * (kappa * kappa) * ((2 + t * t) / (t * t)) * scala.math.log  (12 * H * n / delta)\n",
    "\n",
    "val max_rows = 5000\n",
    "\n",
    "val base_beta = scala.math.min (q2 * kappa * kappa / (base_lamb * n), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:49:35.947644Z",
     "start_time": "2019-04-10T18:49:11.796Z"
    },
    "hidden": true
   },
   "source": [
    "var lambda_h = base_lamb\n",
    "var beta_h = base_beta\n",
    "\n",
    "//step 0, i.e. initialization step\n",
    "var df_sample_set_h:org.apache.spark.sql.Dataset[\n",
    "    org.apache.spark.sql.Row\n",
    "] = df_data.select (\"nfeatures\").sample (beta_h).withColumn (\n",
    "    \"ph_cap\",\n",
    "    LinalgUtils.cap_to_one_udf (\n",
    "        // first round does not have a kpack\n",
    "        // with most kernels, kinda meaningless...\n",
    "        KernelPack.knorm_factory (kernel_func (_,_))($\"nfeatures\").multiply (q2)\n",
    "    )\n",
    ").filter (\n",
    "    s_sql.functions.rand ().multiply (beta_h) <= $\"ph_cap\"\n",
    ").orderBy (\"ph_cap\").limit (max_rows)\n",
    "\n",
    "// sample length\n",
    "// runs most of the computation, lol\n",
    "var cols = df_sample_set_h.count ().toInt\n",
    "\n",
    "val kpack_h = KernelPack (\n",
    "    // X\n",
    "    new breeze.linalg.DenseMatrix (\n",
    "        rows, cols, \n",
    "        df_sample_set_h.select (\"nfeatures\").flatMap {\n",
    "            case row: s_sql.Row => row.getSeq[Double](0)\n",
    "        }.collect ().toArray\n",
    "    ), \n",
    "    // A\n",
    "    new breeze.linalg.DenseVector (\n",
    "        df_sample_set_h.select (\"ph_cap\").map {\n",
    "            case row: s_sql.Row => row.getDouble(0)\n",
    "        }.collect ().toArray\n",
    "    ), \n",
    "    // lambda * n\n",
    "    lambda_h * n, \n",
    "    kernel_func (_,_)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:49:36.502641Z",
     "start_time": "2019-04-10T18:49:11.798Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "source": [
    "lambda_h = base_lamb\n",
    "beta_h = base_beta\n",
    "\n",
    "for (i <- 1 until H) {\n",
    "    println (\"----------------------------------------------\")\n",
    "    println (f\"Step: ${i}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    // step i\n",
    "    // lambda and beta updates\n",
    "    lambda_h /= q\n",
    "    beta_h = scala.math.min (q2 * kappa * kappa / (lambda_h * n), 1.0)\n",
    "    \n",
    "    println (f\"lambda_h: ${lambda_h}, beta_h: ${beta_h}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    // new sample\n",
    "    df_sample_set_h = df_data.select (\"nfeatures\").sample (beta_h).withColumn (\n",
    "        \"ph_cap\",\n",
    "        LinalgUtils.cap_to_one_udf (kpack_h.kleverage_udf ($\"nfeatures\").multiply (q2))\n",
    "    ).filter (\n",
    "        s_sql.functions.rand ().multiply (beta_h) <= $\"ph_cap\"\n",
    "    ).orderBy (s_sql.functions.negate ($\"ph_cap\")).limit (max_rows)\n",
    "\n",
    "    // sample length\n",
    "    // runs most of the computation, lol\n",
    "    cols = df_sample_set_h.count ().toInt\n",
    "\n",
    "    // kernel udfs update\n",
    "    kpack_h.update (\n",
    "        // X\n",
    "        new breeze.linalg.DenseMatrix (\n",
    "            rows, cols, \n",
    "            df_sample_set_h.select (\"nfeatures\").flatMap {\n",
    "                case row: s_sql.Row => row.getSeq[Double](0)\n",
    "            }.collect ().toArray\n",
    "        ), \n",
    "        // A\n",
    "        new breeze.linalg.DenseVector (\n",
    "            df_sample_set_h.select (\"ph_cap\").map {\n",
    "                case row: s_sql.Row => row.getDouble(0)\n",
    "            }.collect ().toArray\n",
    "        ), \n",
    "        // lambda * n\n",
    "        lambda_h * n\n",
    "    )\n",
    "    \n",
    "    println (f\"cols: ${cols}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    // global leverage score\n",
    "    df_data.select (\n",
    "        kpack_h.kleverage_udf ($\"nfeatures\").as (\"ph\")\n",
    "    ).agg (s_sql.functions.sum ($\"ph\")).show\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nystrom spaces loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:52:36.442649Z",
     "start_time": "2019-04-22T16:51:06.780Z"
    }
   },
   "outputs": [],
   "source": [
    "val kappa = 1.0\n",
    "val t = 1.0\n",
    "val base_lamb = kappa * kappa / scala.math.min (t, 1.0)\n",
    "val q = 2\n",
    "// target lambda is 1 / sqrt (n)\n",
    "val H = (scala.math.log (base_lamb * scala.math.sqrt (n)) / scala.math.log (q)).toInt\n",
    "val delta = .9\n",
    "\n",
    "// val q2 = 54 * (kappa * kappa) * ((2 + t * t) / (t * t)) * scala.math.log  (12 * H * n / delta)\n",
    "val q2 = (kappa * kappa) * ((2 + t * t) / (t * t)) * scala.math.log  (12 * H * n / delta)\n",
    "\n",
    "val max_rows = 1000\n",
    "val max_spaces = 4\n",
    "val max_non_improv = 10\n",
    "\n",
    "val base_beta = scala.math.min (q2 * kappa * kappa / (base_lamb), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:52:52.795435Z",
     "start_time": "2019-04-22T16:51:06.783Z"
    }
   },
   "outputs": [],
   "source": [
    "var lambda_h = base_lamb\n",
    "var beta_h = base_beta\n",
    "\n",
    "//step 0, i.e. initialization step\n",
    "var df_sample_set_h:org.apache.spark.sql.Dataset[\n",
    "    org.apache.spark.sql.Row\n",
    "] = df_data.select ($\"nfeatures\", $\"id\").sample (beta_h).withColumn (\n",
    "    \"ph_cap\",\n",
    "    LinalgUtils.cap_to_one_udf (\n",
    "        // first round does not have a kpack\n",
    "        // with most kernels, kinda meaningless...\n",
    "        KernelPack.knorm_factory (kernel_func (_,_))($\"nfeatures\").multiply (scala.math.log  (12 * H * n / delta))\n",
    "    )\n",
    ").filter (\n",
    "    s_sql.functions.rand () <= LinalgUtils.cap_to_one_udf ($\"ph_cap\")\n",
    ").orderBy (s_sql.functions.negate ($\"ph_cap\")).limit (max_rows).persist\n",
    "\n",
    "// sample length\n",
    "var cols = df_sample_set_h.count ().toInt\n",
    "\n",
    "val kspaces_h = KernelSpaces (\n",
    "    // X\n",
    "    new breeze.linalg.DenseMatrix (\n",
    "        rows, cols, \n",
    "        df_sample_set_h.select (\"nfeatures\").flatMap {\n",
    "            case row: s_sql.Row => row.getSeq[Double](0)\n",
    "        }.collect ().toArray\n",
    "    ), \n",
    "    // A\n",
    "    new breeze.linalg.DenseVector (\n",
    "        df_sample_set_h.select (\n",
    "            LinalgUtils.cap_to_one_udf ($\"ph_cap\")\n",
    "        ).map {\n",
    "            case row: s_sql.Row => row.getDouble(0)\n",
    "        }.collect ().toArray\n",
    "    ),\n",
    "    // X_ids\n",
    "    df_sample_set_h.select (\n",
    "        $\"id\"\n",
    "    ).map {\n",
    "        case row: s_sql.Row => row.getLong(0)\n",
    "    }.collect (),\n",
    "    // lambda * n\n",
    "    lambda_h,// * n, \n",
    "    kernel_func (_,_),\n",
    "    max_spaces\n",
    ")\n",
    "\n",
    "df_sample_set_h = df_sample_set_h.unpersist (true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:54:25.192884Z",
     "start_time": "2019-04-22T16:51:06.785Z"
    }
   },
   "outputs": [],
   "source": [
    "var non_improv_count = 0\n",
    "var min_leverage:Double = df_data.agg (\n",
    "    s_sql.functions.sum (kspaces_h.min_kleverage_udf () ($\"nfeatures\").as (\"best_ph\"))\n",
    ").map {\n",
    "    case row: s_sql.Row => row.getDouble(0)\n",
    "}.collect ()(0)\n",
    "\n",
    "var current_leverage:Double = 0.0\n",
    "\n",
    "var best_Xs = kspaces_h.Xs\n",
    "var best_sqrt_GXs = kspaces_h.sqrt_GXs\n",
    "var best_regularizer_diags = kspaces_h.regularizer_diags\n",
    "var best_ids = kspaces_h.Xs_ids\n",
    "\n",
    "var i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:54:26.218329Z",
     "start_time": "2019-04-22T16:51:06.789Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "lambda_h = base_lamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retain best current set of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-22T16:51:06.792Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while (non_improv_count < max_non_improv) {\n",
    "    println (\"----------------------------------------------\")\n",
    "    println (f\"i: ${i}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    // step i\n",
    "    // lambda and beta updates\n",
    "    \n",
    "    if (i < H) {\n",
    "        lambda_h /= q\n",
    "    }\n",
    "\n",
    "    beta_h = scala.math.min (q2 * kappa * kappa / (lambda_h), 1.0)\n",
    "    \n",
    "    println (f\"lambda_h: ${lambda_h}, beta_h: ${beta_h}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    // removal of subspaces when array gets full\n",
    "    if (kspaces_h.Xs_length <= i) {\n",
    "        println (f\"removed a X at i: ${(i - 1) % (kspaces_h.Xs_length - 1)}\")\n",
    "        println (\"----------------------------------------------\")\n",
    "        kspaces_h.remove_id ((i - 1) % (kspaces_h.Xs_length - 1))\n",
    "    }\n",
    "    \n",
    "    kspaces_h.update_lambda (\n",
    "        // lambda * n\n",
    "        lambda_h// * n\n",
    "    )\n",
    "    \n",
    "    // new sample\n",
    "    df_sample_set_h = df_data.select ($\"nfeatures\", $\"id\").sample (beta_h).withColumn (\n",
    "        \"ph_cap\",\n",
    "        kspaces_h.min_kleverage_udf () ($\"nfeatures\").multiply (scala.math.log  (12 * H * n / delta))\n",
    "    ).filter (\n",
    "        s_sql.functions.rand () <= LinalgUtils.cap_to_one_udf ($\"ph_cap\")\n",
    "    ).orderBy (s_sql.functions.negate ($\"ph_cap\")).limit (max_rows).persist\n",
    "\n",
    "    // sample length\n",
    "    cols = df_sample_set_h.count ().toInt\n",
    "\n",
    "    // kernel udfs update\n",
    "    kspaces_h.append (\n",
    "        // X\n",
    "        new breeze.linalg.DenseMatrix (\n",
    "            rows, cols, \n",
    "            df_sample_set_h.select (\"nfeatures\").flatMap {\n",
    "                case row: s_sql.Row => row.getSeq[Double](0)\n",
    "            }.collect ().toArray\n",
    "        ), \n",
    "        // A\n",
    "        new breeze.linalg.DenseVector (\n",
    "            df_sample_set_h.select (\n",
    "                LinalgUtils.cap_to_one_udf ($\"ph_cap\")\n",
    "            ).map {\n",
    "                case row: s_sql.Row => row.getDouble(0)\n",
    "            }.collect ().toArray\n",
    "        ),\n",
    "        // X_ids\n",
    "        df_sample_set_h.select (\n",
    "            $\"id\"\n",
    "        ).map {\n",
    "            case row: s_sql.Row => row.getLong(0)\n",
    "        }.collect ()\n",
    "    )\n",
    "    \n",
    "    println (f\"cols: ${cols}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    df_sample_set_h = df_sample_set_h.unpersist (true)\n",
    "    \n",
    "    println (\"Cross leverages between the Xis\")\n",
    "    for (\n",
    "        k <- 0 until kspaces_h.Xs_count\n",
    "    ) {\n",
    "        print (k)\n",
    "        print (\"    \")\n",
    "\n",
    "        for (l <- 0 until kspaces_h.Xs_count) {\n",
    "            print (f\"\"\"${\n",
    "                breeze.linalg.sum (KernelUtils.kernel_leverages (\n",
    "                    kspaces_h.Xs ()(k),\n",
    "                    kspaces_h.Xs ()(l),\n",
    "                    kspaces_h.sqrt_GXs ()(k),\n",
    "                    kspaces_h.kernel_func\n",
    "                ))\n",
    "            }%.2f\"\"\")\n",
    "            print (\" | \")\n",
    "        }\n",
    "\n",
    "        print (\"\\n\")\n",
    "    }\n",
    "    \n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    current_leverage = df_data.agg (\n",
    "        s_sql.functions.sum (kspaces_h.min_kleverage_udf () ($\"nfeatures\").as (\"best_ph\"))\n",
    "    ).map {\n",
    "        case row: s_sql.Row => row.getDouble(0)\n",
    "    }.collect ()(0)\n",
    "    \n",
    "    println (f\"current leverage sum: ${current_leverage}, current best ${min_leverage}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    if (current_leverage < min_leverage) {\n",
    "        println (f\"improvement happend!\")\n",
    "        println (\"----------------------------------------------\")\n",
    "        min_leverage = current_leverage\n",
    "        // makes so that it stops when \n",
    "        // no improvement happens\n",
    "        // max_non_improv times\n",
    "        if (non_improv_count > 0) {\n",
    "            non_improv_count -= 1\n",
    "        } \n",
    "        best_Xs = kspaces_h.Xs\n",
    "        best_sqrt_GXs = kspaces_h.sqrt_GXs\n",
    "        best_regularizer_diags = kspaces_h.regularizer_diags\n",
    "        best_ids = kspaces_h.Xs_ids\n",
    "    }\n",
    "    else {\n",
    "        non_improv_count += 1\n",
    "        println (f\"current non-improvement: ${non_improv_count}\")\n",
    "        println (\"----------------------------------------------\")\n",
    "    }\n",
    "    \n",
    "    i += 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best's curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-22T16:51:06.796Z"
    }
   },
   "outputs": [],
   "source": [
    "best_Xs.foreach {\n",
    "    case aX => println (aX.cols)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-22T16:51:06.798Z"
    }
   },
   "outputs": [],
   "source": [
    "for (i <- 0 until best_Xs.length) {\n",
    "    print (i)\n",
    "    print (\"    \")\n",
    "\n",
    "    for (j <- 0 until best_Xs.length) {\n",
    "        print (f\"\"\"${\n",
    "            breeze.linalg.sum (KernelUtils.kernel_leverages (\n",
    "                best_Xs (i),\n",
    "                best_Xs (j),\n",
    "                best_sqrt_GXs (i),\n",
    "                kspaces_h.kernel_func\n",
    "            ))\n",
    "        }%.2f\"\"\")\n",
    "        print (\" | \")\n",
    "    }\n",
    "\n",
    "    print (\"\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate leverage scores w.r.t each subspace\n",
    "\n",
    "evaluate 'linear independence' between the subspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-22T16:51:06.801Z"
    }
   },
   "outputs": [],
   "source": [
    "val chosen_ids = Set (best_ids.flatMap {case a => a}:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-22T16:51:06.803Z"
    }
   },
   "outputs": [],
   "source": [
    "val is_chosen_udf = s_sql.functions.udf (\n",
    "    (id:Long) => { chosen_ids (id) }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-22T16:51:06.805Z"
    }
   },
   "outputs": [],
   "source": [
    "val df_chosen = df_data.filter (is_chosen_udf ($\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-22T16:51:06.807Z"
    }
   },
   "outputs": [],
   "source": [
    "df_chosen.groupBy ($\"label\").count ().show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test classification of subset and evaluate generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T19:59:28.684608Z",
     "start_time": "2019-04-25T19:58:53.342Z"
    }
   },
   "outputs": [],
   "source": [
    "%Truncation on\n",
    "%AddDeps org.scalanlp breeze_2.12 1.0-RC2 --transitive --trace --verbose\n",
    "%AddDeps org.scalanlp breeze-natives_2.12 1.0-RC2 --transitive --trace --verbose\n",
    "%AddDeps org.scalanlp breeze-viz_2.12 1.0-RC2 --transitive --trace --verbose\n",
    "%ShowTypes on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T19:59:28.936361Z",
     "start_time": "2019-04-25T19:58:53.344Z"
    }
   },
   "outputs": [],
   "source": [
    "import kernel_lib._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T19:59:29.264956Z",
     "start_time": "2019-04-25T19:58:53.346Z"
    }
   },
   "outputs": [],
   "source": [
    "import com.github.fommil.netlib.LAPACK.{getInstance => lapack}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## kernel func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T19:59:30.672000Z",
     "start_time": "2019-04-25T19:58:53.348Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def kernel_func (\n",
    "    x:breeze.linalg.DenseVector[Double], \n",
    "    y:breeze.linalg.DenseVector[Double]\n",
    "):Double = x.t * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Spark Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T20:35:43.729288Z",
     "start_time": "2019-05-27T20:35:43.578Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.{sql => s_sql}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T21:11:31.955930Z",
     "start_time": "2019-05-27T21:11:31.451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val csv_to_label_features_udf = s_sql.functions.udf (\n",
    "    (row:String) => {\n",
    "        val label_features = row.split (',')\n",
    "        \n",
    "        (label_features (0).toDouble.toLong, label_features.slice (1, row.length).iterator.map (_.toDouble).toSeq)\n",
    "    }:(Long, Seq[Double])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T19:59:31.818527Z",
     "start_time": "2019-04-25T19:58:53.353Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val max_per_P = 2000\n",
    "\n",
    "val path = \"hdfs://thesis-tiny-python3-anaconda-m/user/<your username>/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T21:11:36.634230Z",
     "start_time": "2019-05-27T21:11:36.181Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var df_susy_raw = spark.read.format (\n",
    "    \"text\"\n",
    ").load (path + \"SUSY.csv.gz\")\n",
    ".filter (\n",
    "    !$\"value\".contains (\"MET_rel\")\n",
    ").withColumn (\n",
    "    \"label_features\", csv_to_label_features_udf ($\"value\")\n",
    ").select (\n",
    "    $\"label_features._1\".as (\"label\"),\n",
    "    $\"label_features._2\".as (\"features\")\n",
    ").withColumn (\n",
    "    \"id\", s_sql.functions.monotonically_increasing_id\n",
    ")\n",
    "\n",
    "df_susy_raw = df_susy_raw.repartition (df_susy_raw.count ().toInt / max_per_P)\n",
    "\n",
    "df_susy_raw.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-27T21:11:36.634230Z",
     "start_time": "2019-05-27T21:11:36.181Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var df_higgs = spark.read.format (\n",
    "    \"text\"\n",
    ").load (path + \"HIGGS.csv.gz\")\n",
    ".filter (\n",
    "    !$\"value\".contains (\"m_jj\")\n",
    ").withColumn (\n",
    "    \"label_features\", csv_to_label_features_udf ($\"value\")\n",
    ").select (\n",
    "    $\"label_features._1\".as (\"label\"),\n",
    "    $\"label_features._2\".as (\"features\")\n",
    ").withColumn (\n",
    "    \"id\", s_sql.functions.monotonically_increasing_id\n",
    ")\n",
    "\n",
    "df_higgs_raw = df_higgs_raw.repartition (df_higgs_raw.count ().toInt / max_per_P)\n",
    "\n",
    "df_higgs_raw.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T19:59:44.570722Z",
     "start_time": "2019-04-25T19:58:53.355Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var df_digits_raw = spark.read.format (\n",
    "    \"parquet\"\n",
    ").load (path + \"digits.snappy.parquet\").withColumn (\n",
    "    \"features\", $\"features\".cast (\"ARRAY<DOUBLE>\")\n",
    ").withColumn (\n",
    "    \"id\", s_sql.functions.monotonically_increasing_id\n",
    ")\n",
    "\n",
    "df_digits_raw = df_digits_raw.repartition (df_digits_raw.count ().toInt / max_per_P)\n",
    "\n",
    "df_digits_raw.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nystrom algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Features transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T19:59:58.518248Z",
     "start_time": "2019-04-25T19:58:53.359Z"
    },
    "hidden": true
   },
   "source": [
    "```scala\n",
    "val image_dim = scala.math.sqrt (\n",
    "    df_digits_raw.select (\n",
    "        s_sql.functions.size ($\"features\")\n",
    "    ).map (_.getInt (0)).take (1)(0)\n",
    ").toInt\n",
    "\n",
    "// window is centered\n",
    "\n",
    "val window_x = 7\n",
    "val window_y = 7\n",
    "val skip_x = 3\n",
    "val skip_y = 3\n",
    "\n",
    "def max_pool (\n",
    "    I:breeze.linalg.DenseMatrix[Double]\n",
    "):breeze.linalg.DenseMatrix[Double] = {\n",
    "    // column major implies image will be\n",
    "    val I_pooled = new breeze.linalg.DenseMatrix[Double](\n",
    "        (I.rows - window_y) / skip_y, \n",
    "        (I.cols - window_x) / skip_x\n",
    "    )\n",
    "    \n",
    "    for (\n",
    "        (i, im, ip) <- (0 until I_pooled.rows).map (\n",
    "            (v:Int) => (v, skip_y * v, skip_y * v + window_y)\n",
    "        );\n",
    "        (j, jm, jp) <- (0 until I_pooled.cols).map (\n",
    "            (v:Int) => (v, skip_x * v, skip_x * v + window_x)\n",
    "        )\n",
    "    ) {\n",
    "        I_pooled (i, j) = breeze.linalg.max (I (im until ip, jm until jp))\n",
    "    }\n",
    "    \n",
    "    I_pooled\n",
    "}\n",
    "\n",
    "val max_pool_udf = s_sql.functions.udf (\n",
    "    (x:Seq[Double]) => {\n",
    "        max_pool (\n",
    "            new breeze.linalg.DenseMatrix (image_dim, image_dim, x.toArray)\n",
    "        ).data.toSeq\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T20:00:04.345215Z",
     "start_time": "2019-04-25T19:58:53.363Z"
    }
   },
   "outputs": [],
   "source": [
    "val df_data = df_digits_raw.select (\n",
    "    LinalgUtils.normalize_udf (\n",
    "        $\"features\"\n",
    "    ).as (\"nfeatures\"), $\"label\", $\"id\"\n",
    ").persist\n",
    "\n",
    "// mnist\n",
    "val df_data = df_digits_raw.select (\n",
    "    LinalgUtils.normalize_udf (\n",
    "        $\"features\"\n",
    "    ).as (\"nfeatures\"), $\"label\", $\"id\"\n",
    ").persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T20:00:20.809721Z",
     "start_time": "2019-04-25T19:58:53.365Z"
    }
   },
   "outputs": [],
   "source": [
    "val n = df_data.count ().toInt\n",
    "val rows = df_data.select (\n",
    "    s_sql.functions.size ($\"nfeatures\")\n",
    ").map (_.getInt (0)).take (1)(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Nystrom Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:49:35.356678Z",
     "start_time": "2019-04-10T18:49:11.795Z"
    },
    "hidden": true
   },
   "source": [
    "```scala\n",
    "val kappa = 1.0\n",
    "val t = .3\n",
    "val base_lamb = kappa * kappa / scala.math.min (t, 1.0)\n",
    "val q = 1.5\n",
    "// target lambda is 1 / sqrt (n)\n",
    "val H = (scala.math.log (base_lamb * scala.math.sqrt (n)) / scala.math.log (q)).toInt\n",
    "val delta = .9\n",
    "\n",
    "val q2 = 54 * (kappa * kappa) * ((2 + t * t) / (t * t)) * scala.math.log  (12 * H * n / delta)\n",
    "\n",
    "val max_rows = 5000\n",
    "\n",
    "val base_beta = scala.math.min (q2 * kappa * kappa / (base_lamb * n), 1.0)\n",
    "\n",
    "var lambda_h = base_lamb\n",
    "var beta_h = base_beta\n",
    "\n",
    "//step 0, i.e. initialization step\n",
    "var df_sample_set_h:org.apache.spark.sql.Dataset[\n",
    "    org.apache.spark.sql.Row\n",
    "] = df_data.select (\"nfeatures\").sample (beta_h).withColumn (\n",
    "    \"ph_cap\",\n",
    "    LinalgUtils.cap_to_one_udf (\n",
    "        // first round does not have a kpack\n",
    "        // with most kernels, kinda meaningless...\n",
    "        KernelPack.knorm_factory (kernel_func (_,_))($\"nfeatures\").multiply (q2)\n",
    "    )\n",
    ").filter (\n",
    "    s_sql.functions.rand ().multiply (beta_h) <= $\"ph_cap\"\n",
    ").orderBy (\"ph_cap\").limit (max_rows)\n",
    "\n",
    "// sample length\n",
    "// runs most of the computation, lol\n",
    "var cols = df_sample_set_h.count ().toInt\n",
    "\n",
    "val kpack_h = KernelPack (\n",
    "    // X\n",
    "    new breeze.linalg.DenseMatrix (\n",
    "        rows, cols, \n",
    "        df_sample_set_h.select (\"nfeatures\").flatMap {\n",
    "            case row: s_sql.Row => row.getSeq[Double](0)\n",
    "        }.collect ().toArray\n",
    "    ), \n",
    "    // A\n",
    "    new breeze.linalg.DenseVector (\n",
    "        df_sample_set_h.select (\"ph_cap\").map {\n",
    "            case row: s_sql.Row => row.getDouble(0)\n",
    "        }.collect ().toArray\n",
    "    ), \n",
    "    // lambda * n\n",
    "    lambda_h * n, \n",
    "    kernel_func (_,_)\n",
    ")\n",
    "\n",
    "lambda_h = base_lamb\n",
    "beta_h = base_beta\n",
    "\n",
    "for (i <- 1 until H) {\n",
    "    println (\"----------------------------------------------\")\n",
    "    println (f\"Step: ${i}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    // step i\n",
    "    // lambda and beta updates\n",
    "    lambda_h /= q\n",
    "    beta_h = scala.math.min (q2 * kappa * kappa / (lambda_h * n), 1.0)\n",
    "    \n",
    "    println (f\"lambda_h: ${lambda_h}, beta_h: ${beta_h}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    // new sample\n",
    "    df_sample_set_h = df_data.select (\"nfeatures\").sample (beta_h).withColumn (\n",
    "        \"ph_cap\",\n",
    "        LinalgUtils.cap_to_one_udf (kpack_h.kleverage_udf ($\"nfeatures\").multiply (q2))\n",
    "    ).filter (\n",
    "        s_sql.functions.rand ().multiply (beta_h) <= $\"ph_cap\"\n",
    "    ).orderBy (s_sql.functions.negate ($\"ph_cap\")).limit (max_rows)\n",
    "\n",
    "    // sample length\n",
    "    // runs most of the computation, lol\n",
    "    cols = df_sample_set_h.count ().toInt\n",
    "\n",
    "    // kernel udfs update\n",
    "    kpack_h.update (\n",
    "        // X\n",
    "        new breeze.linalg.DenseMatrix (\n",
    "            rows, cols, \n",
    "            df_sample_set_h.select (\"nfeatures\").flatMap {\n",
    "                case row: s_sql.Row => row.getSeq[Double](0)\n",
    "            }.collect ().toArray\n",
    "        ), \n",
    "        // A\n",
    "        new breeze.linalg.DenseVector (\n",
    "            df_sample_set_h.select (\"ph_cap\").map {\n",
    "                case row: s_sql.Row => row.getDouble(0)\n",
    "            }.collect ().toArray\n",
    "        ), \n",
    "        // lambda * n\n",
    "        lambda_h * n\n",
    "    )\n",
    "    \n",
    "    println (f\"cols: ${cols}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    // global leverage score\n",
    "    df_data.select (\n",
    "        kpack_h.kleverage_udf ($\"nfeatures\").as (\"ph\")\n",
    "    ).agg (s_sql.functions.sum ($\"ph\")).show\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Nystrom spaces loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T20:00:26.112410Z",
     "start_time": "2019-04-25T19:58:53.368Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val kappa = 1.0\n",
    "val t = 1.0\n",
    "val base_lamb = kappa * kappa / scala.math.min (t, 1.0)\n",
    "val q = 2\n",
    "// target lambda is 1 / n\n",
    "val H = (scala.math.log (base_lamb * n) / scala.math.log (q)).toInt\n",
    "val delta = .9\n",
    "\n",
    "// val q2 = 54 * (kappa * kappa) * ((2 + t * t) / (t * t)) * scala.math.log  (12 * H * n / delta)\n",
    "val q2 = (kappa * kappa) * ((2 + t * t) / (t * t)) * scala.math.log  (12 * H * n / delta)\n",
    "\n",
    "val max_rows = 1000\n",
    "val max_spaces = 5\n",
    "val max_non_improv = 10\n",
    "\n",
    "val base_beta = scala.math.min (q2 * kappa * kappa / (base_lamb), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T20:00:44.632855Z",
     "start_time": "2019-04-25T19:58:53.370Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var lambda_h = base_lamb\n",
    "var beta_h = base_beta\n",
    "\n",
    "//step 0, i.e. initialization step\n",
    "var df_sample_set_h:org.apache.spark.sql.Dataset[\n",
    "    org.apache.spark.sql.Row\n",
    "] = df_data.select ($\"nfeatures\", $\"id\").sample (beta_h).withColumn (\n",
    "    \"ph_cap\",\n",
    "    LinalgUtils.cap_to_one_udf (\n",
    "        // first round does not have a kpack\n",
    "        // with most kernels, kinda meaningless...\n",
    "        KernelPack.knorm_factory (kernel_func (_,_))($\"nfeatures\").multiply (scala.math.log  (12 * H * n / delta))\n",
    "    )\n",
    ").filter (\n",
    "    s_sql.functions.rand () <= LinalgUtils.cap_to_one_udf ($\"ph_cap\")\n",
    ").orderBy (s_sql.functions.negate ($\"ph_cap\")).limit (max_rows).persist\n",
    "\n",
    "// sample length\n",
    "var cols = df_sample_set_h.count ().toInt\n",
    "\n",
    "val kspaces_h = KernelSpaces (\n",
    "    // X\n",
    "    new breeze.linalg.DenseMatrix (\n",
    "        rows, cols, \n",
    "        df_sample_set_h.select (\"nfeatures\").flatMap {\n",
    "            case row: s_sql.Row => row.getSeq[Double](0)\n",
    "        }.collect ().toArray\n",
    "    ), \n",
    "    // A\n",
    "    new breeze.linalg.DenseVector (\n",
    "        df_sample_set_h.select (\n",
    "            LinalgUtils.cap_to_one_udf ($\"ph_cap\")\n",
    "        ).map {\n",
    "            case row: s_sql.Row => row.getDouble(0)\n",
    "        }.collect ().toArray\n",
    "    ),\n",
    "    // X_ids\n",
    "    df_sample_set_h.select (\n",
    "        $\"id\"\n",
    "    ).map {\n",
    "        case row: s_sql.Row => row.getLong(0)\n",
    "    }.collect (),\n",
    "    // lambda * n\n",
    "    lambda_h,// * n, \n",
    "    kernel_func (_,_),\n",
    "    max_spaces\n",
    ")\n",
    "\n",
    "df_sample_set_h = df_sample_set_h.unpersist (true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T20:02:21.054851Z",
     "start_time": "2019-04-25T19:58:53.373Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var non_improv_count = 0\n",
    "var min_leverage:Double = df_data.agg (\n",
    "    s_sql.functions.sum (kspaces_h.min_kleverage_udf () ($\"nfeatures\").as (\"best_ph\"))\n",
    ").map {\n",
    "    case row: s_sql.Row => row.getDouble(0)\n",
    "}.collect ()(0)\n",
    "\n",
    "var current_leverage:Double = 0.0\n",
    "\n",
    "var best_Xs = kspaces_h.Xs\n",
    "var best_sqrt_GXs = kspaces_h.sqrt_GXs\n",
    "var best_regularizer_diags = kspaces_h.regularizer_diags\n",
    "var best_ids = kspaces_h.Xs_ids\n",
    "\n",
    "var i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T20:02:22.338793Z",
     "start_time": "2019-04-25T19:58:53.375Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "lambda_h = base_lamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "devise map partitions strategy by using mapPartitions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T22:12:23.947128Z",
     "start_time": "2019-04-25T19:58:53.379Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while (non_improv_count < max_non_improv) {\n",
    "    println (\"----------------------------------------------\")\n",
    "    println (f\"i: ${i}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    // step i\n",
    "    // lambda and beta updates\n",
    "    \n",
    "    if (i < H) {\n",
    "        lambda_h /= q\n",
    "    }\n",
    "\n",
    "    beta_h = scala.math.min (q2 * kappa * kappa / (lambda_h), 1.0)\n",
    "    \n",
    "    println (f\"lambda_h: ${lambda_h}, beta_h: ${beta_h}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    // removal of subspaces when array gets full\n",
    "    if (kspaces_h.Xs_length <= i) {\n",
    "        println (f\"removed a X at i: ${(i - 1) % (kspaces_h.Xs_length - 1)}\")\n",
    "        println (\"----------------------------------------------\")\n",
    "        kspaces_h.remove_id ((i - 1) % (kspaces_h.Xs_length - 1))\n",
    "    }\n",
    "    \n",
    "    kspaces_h.update_lambda (\n",
    "        // lambda * n\n",
    "        lambda_h// * n\n",
    "    )\n",
    "    \n",
    "    // new sample\n",
    "    df_sample_set_h = df_data.select ($\"nfeatures\", $\"id\").sample (beta_h).withColumn (\n",
    "        \"ph_cap\",\n",
    "        kspaces_h.min_kleverage_udf () ($\"nfeatures\").multiply (scala.math.log  (12 * H * n / delta))\n",
    "    ).filter (\n",
    "        s_sql.functions.rand () <= LinalgUtils.cap_to_one_udf ($\"ph_cap\")\n",
    "    ).orderBy (s_sql.functions.negate ($\"ph_cap\")).limit (max_rows).persist\n",
    "\n",
    "    // sample length\n",
    "    cols = df_sample_set_h.count ().toInt\n",
    "\n",
    "    // kernel udfs update\n",
    "    kspaces_h.append (\n",
    "        // X\n",
    "        new breeze.linalg.DenseMatrix (\n",
    "            rows, cols, \n",
    "            df_sample_set_h.select (\"nfeatures\").flatMap {\n",
    "                case row: s_sql.Row => row.getSeq[Double](0)\n",
    "            }.collect ().toArray\n",
    "        ), \n",
    "        // A\n",
    "        new breeze.linalg.DenseVector (\n",
    "            df_sample_set_h.select (\n",
    "                LinalgUtils.cap_to_one_udf ($\"ph_cap\")\n",
    "            ).map {\n",
    "                case row: s_sql.Row => row.getDouble(0)\n",
    "            }.collect ().toArray\n",
    "        ),\n",
    "        // X_ids\n",
    "        df_sample_set_h.select (\n",
    "            $\"id\"\n",
    "        ).map {\n",
    "            case row: s_sql.Row => row.getLong(0)\n",
    "        }.collect ()\n",
    "    )\n",
    "    \n",
    "    println (f\"cols: ${cols}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    df_sample_set_h = df_sample_set_h.unpersist (true)\n",
    "    \n",
    "    println (\"Cross leverages between the Xis\")\n",
    "    for (\n",
    "        k <- 0 until kspaces_h.Xs_count\n",
    "    ) {\n",
    "        print (k)\n",
    "        print (\"    \")\n",
    "\n",
    "        for (l <- 0 until kspaces_h.Xs_count) {\n",
    "            print (f\"\"\"${\n",
    "                breeze.linalg.sum (KernelUtils.kernel_leverages (\n",
    "                    kspaces_h.Xs ()(k),\n",
    "                    kspaces_h.Xs ()(l),\n",
    "                    kspaces_h.sqrt_GXs ()(k),\n",
    "                    kspaces_h.kernel_func\n",
    "                ))\n",
    "            }%.2f\"\"\")\n",
    "            print (\" | \")\n",
    "        }\n",
    "\n",
    "        print (\"\\n\")\n",
    "    }\n",
    "    \n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    current_leverage = df_data.agg (\n",
    "        s_sql.functions.sum (kspaces_h.min_kleverage_udf () ($\"nfeatures\").as (\"best_ph\"))\n",
    "    ).map {\n",
    "        case row: s_sql.Row => row.getDouble(0)\n",
    "    }.collect ()(0)\n",
    "    \n",
    "    println (f\"current leverage sum: ${current_leverage}, current best ${min_leverage}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    if (current_leverage - min_leverage < -1) {\n",
    "        println (f\"improvement happend!\")\n",
    "        println (\"----------------------------------------------\")\n",
    "        min_leverage = current_leverage\n",
    "        \n",
    "        // makes so that it stops when \n",
    "        // no improvement happens\n",
    "        // max_non_improv times\n",
    "        if (non_improv_count > 0) {\n",
    "            non_improv_count -= 1\n",
    "        }\n",
    "        \n",
    "        best_Xs = kspaces_h.Xs\n",
    "        best_sqrt_GXs = kspaces_h.sqrt_GXs\n",
    "        best_regularizer_diags = kspaces_h.regularizer_diags\n",
    "        best_ids = kspaces_h.Xs_ids\n",
    "    }\n",
    "    else {\n",
    "        non_improv_count += 1\n",
    "        println (f\"current non-improvement: ${non_improv_count}\")\n",
    "        println (\"----------------------------------------------\")\n",
    "    }\n",
    "    \n",
    "    i += 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Best's curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T22:12:24.260896Z",
     "start_time": "2019-04-25T19:58:53.401Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_Xs.foreach {\n",
    "    case aX => println (aX.cols)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T22:12:32.507897Z",
     "start_time": "2019-04-25T19:58:53.403Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for (i <- 0 until best_Xs.length) {\n",
    "    print (i)\n",
    "    print (\"    \")\n",
    "\n",
    "    for (j <- 0 until best_Xs.length) {\n",
    "        print (f\"\"\"${\n",
    "            breeze.linalg.sum (KernelUtils.kernel_leverages (\n",
    "                best_Xs (i),\n",
    "                best_Xs (j),\n",
    "                best_sqrt_GXs (i),\n",
    "                kspaces_h.kernel_func\n",
    "            ))\n",
    "        }%.2f\"\"\")\n",
    "        print (\" | \")\n",
    "    }\n",
    "\n",
    "    print (\"\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Label spaces, then calssification via spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T22:12:33.476709Z",
     "start_time": "2019-04-25T19:58:53.444Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val chosen_ids = Set (best_ids.flatMap {case a => a}:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T22:12:34.259893Z",
     "start_time": "2019-04-25T19:58:53.447Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val is_chosen_udf = s_sql.functions.udf (\n",
    "    (id:Long) => { chosen_ids (id) }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T22:12:35.063497Z",
     "start_time": "2019-04-25T19:58:53.449Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val df_chosen = df_data.filter (is_chosen_udf ($\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T22:13:02.723847Z",
     "start_time": "2019-04-25T19:58:53.451Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_chosen.groupBy ($\"label\").count ().show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "test classification of subset and evaluate generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T22:13:03.422867Z",
     "start_time": "2019-04-25T19:58:53.453Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val df_split_data = df_chosen.groupBy ($\"label\").agg (\n",
    "    s_sql.functions.collect_list ($\"nfeatures\").as (\"nfeatures\"),\n",
    "    s_sql.functions.count ($\"nfeatures\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T22:13:26.244653Z",
     "start_time": "2019-04-25T19:58:53.455Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val labels_features = df_split_data.map {\n",
    "    case row:s_sql.Row => (row.getLong (0).toInt, row.getSeq[Seq[Double]] (1).flatten, row.getLong (2).toInt)\n",
    "}.collect ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T22:13:29.217426Z",
     "start_time": "2019-04-25T19:58:53.457Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val labels_arr = labels_features.map (_._1).toSeq\n",
    "val features_arr = labels_features.map {\n",
    "    case (_, aX, aX_cols) => new breeze.linalg.DenseMatrix (rows, aX_cols, aX.toArray)\n",
    "}.toSeq\n",
    "\n",
    "val sqrt_Gfeatures_arr = features_arr.map {\n",
    "    case aX => KernelUtils.eval_chol (\n",
    "        aX, breeze.linalg.DenseVector.ones[Double] (aX.cols), \n",
    "        1.0 / scala.math.sqrt (n), kernel_func\n",
    "    )\n",
    "}\n",
    "\n",
    "val labeled_spaces = labels_arr.zip (features_arr).zip (sqrt_Gfeatures_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T22:13:31.166619Z",
     "start_time": "2019-04-25T19:58:53.460Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val buffer_len = features_arr.map (_.cols).max\n",
    "\n",
    "val max_proj_classifier_udf = org.apache.spark.sql.functions.udf (\n",
    "    (y:Seq[Double]) => {\n",
    "        val dense_y = new breeze.linalg.DenseVector (y.toArray)\n",
    "        val buff_y_proj = new breeze.linalg.DenseVector[Double] (buffer_len)\n",
    "\n",
    "        val norm = KernelUtils.kernel_norm (\n",
    "            dense_y, kernel_func\n",
    "        )\n",
    "\n",
    "        var best_proj:Double = 0.0\n",
    "        var best_label:Long = 0\n",
    "        var curr_proj:Double = 0.0\n",
    "        \n",
    "        for (\n",
    "            ((label, aX), sqrt_GX) <- labeled_spaces\n",
    "        ) {\n",
    "            KernelUtils.buff_kernel_proj (\n",
    "                aX, dense_y, sqrt_GX, kernel_func,\n",
    "                buff_y_proj(0 until aX.cols)\n",
    "            )\n",
    "            \n",
    "            curr_proj = buff_y_proj(0 until aX.cols).t * buff_y_proj(0 until aX.cols)\n",
    "            \n",
    "            if (curr_proj >= best_proj) {\n",
    "                best_proj = curr_proj\n",
    "                best_label = label\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        (best_label, norm - best_proj)\n",
    "    }:(Long, Double)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T22:21:23.847430Z",
     "start_time": "2019-04-25T19:58:53.462Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_data.withColumn (\n",
    "    \"pred_pair\", max_proj_classifier_udf ($\"nfeatures\")\n",
    ").select (\n",
    "    $\"label\", $\"pred_pair._1\".as (\"pred_label\"), \n",
    "    $\"pred_pair._2\".as (\"pred_iscore\")\n",
    ").groupBy (\n",
    "    $\"label\".equalTo ($\"pred_label\").as (\"mismatch\")\n",
    ").agg (\n",
    "    s_sql.functions.count (\"*\").as (\"group_sum\"),\n",
    "    s_sql.functions.sum ($\"pred_iscore\").as (\"group_sum\")\n",
    ").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Develop ridge classifier for each subspace, then take majority vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "retrieves labels of each subpace, in the right order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import scala.collection.mutable.Map\n",
    "\n",
    "import org.apache.spark.{ml => s_ml}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-26T15:00:07.580919Z",
     "start_time": "2019-04-26T15:00:05.051Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val best_labels = for (some_ids <- best_ids) yield {\n",
    "    val id_set = Set (some_ids:_*)\n",
    "    val id_map = some_ids.zip (0 until some_ids.length).toMap\n",
    "    \n",
    "    val chosen_udf = s_sql.functions.udf (\n",
    "        (id:Long) => { id_set (id) }\n",
    "    )\n",
    "    \n",
    "    df_data.filter (\n",
    "        chosen_udf ($\"id\")\n",
    "    ).select ($\"label\", $\"id\").map {\n",
    "        case row:s_sql.Row => (row.getLong (0).toInt, row.getLong (1))\n",
    "    }.collect ().sortBy {\n",
    "        case (_, some_id:Long) => id_map (some_id)\n",
    "    }.map (_._1)\n",
    "}.toSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Ridge regression based on subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

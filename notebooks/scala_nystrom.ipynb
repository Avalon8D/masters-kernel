{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:27:38.939122Z",
     "start_time": "2019-04-19T21:26:35.457Z"
    }
   },
   "outputs": [],
   "source": [
    "%Truncation on\n",
    "%AddDeps org.scalanlp breeze_2.12 1.0-RC2 --transitive --trace --verbose\n",
    "%AddDeps org.scalanlp breeze-natives_2.12 1.0-RC2 --transitive --trace --verbose\n",
    "%AddDeps org.scalanlp breeze-viz_2.12 1.0-RC2 --transitive --trace --verbose\n",
    "%ShowTypes on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addapt to importing from jar :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kernel_lib._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:27:39.284309Z",
     "start_time": "2019-04-19T21:26:35.467Z"
    }
   },
   "outputs": [],
   "source": [
    "import com.github.fommil.netlib.LAPACK.{getInstance => lapack}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Linalg UDFs and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:27:44.242835Z",
     "start_time": "2019-04-19T21:26:35.475Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "object LinalgUtils extends Serializable {\n",
    "    def inplace_lower_triangular_solve (\n",
    "        L:breeze.linalg.DenseMatrix[Double], \n",
    "        x: breeze.linalg.DenseVector[Double]\n",
    "    ):Unit = {\n",
    "        val N = L.rows\n",
    "        val info = new  org.netlib.util.intW (0)\n",
    "        lapack.dtrtrs(\n",
    "            \"L\" /* lower triangular */,\n",
    "            \"N\",\n",
    "            \"N\",\n",
    "            N /* number of rows */,\n",
    "            1 /* number of right hand sides */,\n",
    "            L.data,\n",
    "            scala.math.max(1, N) /* LDA */,\n",
    "            x.data,\n",
    "            scala.math.max(1, N) /* LDB */,\n",
    "            info\n",
    "        )\n",
    "        \n",
    "        assert(info.`val` >= 0)\n",
    "\n",
    "        if (info.`val` > 0)\n",
    "        throw new breeze.linalg.MatrixSingularException\n",
    "    }\n",
    "    \n",
    "    def inplace_lower_triangular_solve (\n",
    "        L:breeze.linalg.DenseMatrix[Double], \n",
    "        X: breeze.linalg.DenseMatrix[Double]\n",
    "    ):Unit = {\n",
    "        val N = L.rows\n",
    "        val info = new  org.netlib.util.intW (0)\n",
    "        lapack.dtrtrs(\n",
    "            \"L\" /* lower triangular */,\n",
    "            \"N\",\n",
    "            \"N\",\n",
    "            N /* number of rows */,\n",
    "            X.cols /* number of right hand sides */,\n",
    "            L.data,\n",
    "            scala.math.max(1, N) /* LDA */,\n",
    "            X.data,\n",
    "            scala.math.max(1, N) /* LDB */,\n",
    "            info\n",
    "        )\n",
    "        \n",
    "        assert(info.`val` >= 0)\n",
    "\n",
    "        if (info.`val` > 0)\n",
    "        throw new breeze.linalg.NotConvergedException(breeze.linalg.NotConvergedException.Iterations)\n",
    "    }\n",
    "    \n",
    "    def implace_cholesky (X: breeze.linalg.DenseMatrix[Double]): Unit = {\n",
    "        // set upper triangle to 0\n",
    "        for (i <- 0 until X.rows; j <- i + 1 until X.cols) {\n",
    "            X (i, j) = 0.0\n",
    "        }\n",
    "\n",
    "        val N = X.rows\n",
    "        val info = new  org.netlib.util.intW (0)\n",
    "        lapack.dpotrf(\n",
    "            \"L\" /* lower triangular */,\n",
    "            N /* number of rows */,\n",
    "            X.data,\n",
    "            scala.math.max(1, N) /* LDA */,\n",
    "            info\n",
    "        )\n",
    "        // A value of info.`val` < 0 would tell us that the i-th argument\n",
    "        // of the call to dpotrf was erroneous (where i == |info.`val`|).\n",
    "        assert(info.`val` >= 0)\n",
    "\n",
    "        if (info.`val` > 0)\n",
    "        throw new breeze.linalg.NotConvergedException(breeze.linalg.NotConvergedException.Iterations)\n",
    "    }\n",
    "    \n",
    "    val norm_sqr_udf = org.apache.spark.sql.functions.udf (\n",
    "        (y:Seq[Double]) => {\n",
    "            val dense_y = new breeze.linalg.DenseVector (y.toArray)\n",
    "\n",
    "            dense_y.t * dense_y\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    val norm_udf = org.apache.spark.sql.functions.udf (\n",
    "        (y:Seq[Double]) => {\n",
    "            val dense_y = new breeze.linalg.DenseVector (y.toArray)\n",
    "\n",
    "            breeze.linalg.norm (dense_y)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    val normalize_udf = org.apache.spark.sql.functions.udf (\n",
    "        (y:Seq[Double]) => {\n",
    "            val dense_y = new breeze.linalg.DenseVector (y.toArray)\n",
    "\n",
    "            dense_y /= breeze.linalg.norm (dense_y)\n",
    "            \n",
    "            dense_y.data.toSeq\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    val cap_to_one_udf = org.apache.spark.sql.functions.udf (\n",
    "        scala.math.min (_:Double, 1.0)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Pack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:27:47.771649Z",
     "start_time": "2019-04-19T21:26:35.481Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "object KernelUtils extends Serializable {    \n",
    "    val kernel_norm = (\n",
    "        x:breeze.linalg.DenseVector[Double], \n",
    "        kernel_func:(breeze.linalg.DenseVector[Double], breeze.linalg.DenseVector[Double]) => Double\n",
    "    ) => kernel_func (x, x)\n",
    "\n",
    "    def kernel_cross (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        y:breeze.linalg.DenseVector[Double],\n",
    "        kernel_func:(breeze.linalg.DenseVector[Double], breeze.linalg.DenseVector[Double]) => Double\n",
    "    ):breeze.linalg.DenseVector[Double] = {\n",
    "        X(::, breeze.linalg.*).map (\n",
    "            (x:breeze.linalg.DenseVector[Double]) => kernel_func (x, y)\n",
    "        ).t\n",
    "    }\n",
    "\n",
    "    def kernel_proj (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        y:breeze.linalg.DenseVector[Double],\n",
    "        sqrt_GX:breeze.linalg.DenseMatrix[Double],\n",
    "        kernel_func:(breeze.linalg.DenseVector[Double], breeze.linalg.DenseVector[Double]) => Double\n",
    "    ):breeze.linalg.DenseVector[Double] = {\n",
    "        val y_proj = kernel_cross (X, y, kernel_func)\n",
    "        \n",
    "        LinalgUtils.inplace_lower_triangular_solve (sqrt_GX, y_proj)\n",
    "        \n",
    "        y_proj\n",
    "    }\n",
    "    \n",
    "    // alters out vector's content\n",
    "    def buff_kernel_proj (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        y:breeze.linalg.DenseVector[Double],\n",
    "        sqrt_GX:breeze.linalg.DenseMatrix[Double],\n",
    "        kernel_func:(breeze.linalg.DenseVector[Double], breeze.linalg.DenseVector[Double]) => Double,\n",
    "        out:breeze.linalg.DenseVector[Double]\n",
    "    ):Unit = {\n",
    "        for (i <- 0 until X.cols) {\n",
    "            out (i) = kernel_func (X(::, i), y)\n",
    "        }\n",
    "        \n",
    "        LinalgUtils.inplace_lower_triangular_solve (sqrt_GX, out)\n",
    "    }\n",
    "\n",
    "    def kernel_crosses (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        Y:breeze.linalg.DenseMatrix[Double],\n",
    "        kernel_func:(breeze.linalg.DenseVector[Double], breeze.linalg.DenseVector[Double]) => Double\n",
    "    ):breeze.linalg.DenseMatrix[Double] = {\n",
    "        // does Y then X because column major\n",
    "        val kernel_cross_array = Y(::, breeze.linalg.*).iterator.flatMap (\n",
    "            (y:breeze.linalg.DenseVector[Double]) => {\n",
    "                X(::, breeze.linalg.*).iterator.map (\n",
    "                    (x:breeze.linalg.DenseVector[Double]) => kernel_func (x, y)\n",
    "                )\n",
    "            }\n",
    "        ).toArray\n",
    "\n",
    "        new breeze.linalg.DenseMatrix (\n",
    "            X.cols, kernel_cross_array.length / X.cols, \n",
    "            kernel_cross_array\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    def kernel_projs (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        Y:breeze.linalg.DenseMatrix[Double],\n",
    "        sqrt_GX:breeze.linalg.DenseMatrix[Double],\n",
    "        kernel_func:(breeze.linalg.DenseVector[Double], breeze.linalg.DenseVector[Double]) => Double\n",
    "    ):breeze.linalg.DenseMatrix[Double] = {\n",
    "        val Y_proj = kernel_crosses (X, Y, kernel_func)\n",
    "        \n",
    "        LinalgUtils.inplace_lower_triangular_solve (sqrt_GX, Y_proj)\n",
    "        \n",
    "        Y_proj\n",
    "    }\n",
    "    \n",
    "    def kernel_leverages (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        Y:breeze.linalg.DenseMatrix[Double],\n",
    "        sqrt_GX:breeze.linalg.DenseMatrix[Double],\n",
    "        kernel_func:(breeze.linalg.DenseVector[Double], breeze.linalg.DenseVector[Double]) => Double\n",
    "    ):breeze.linalg.DenseVector[Double] = {\n",
    "        val Y_proj = kernel_projs (X, Y, sqrt_GX, kernel_func)\n",
    "        \n",
    "        Y_proj :*= Y_proj\n",
    "        \n",
    "        val residual = breeze.linalg.sum (Y_proj.t (breeze.linalg.*, ::))\n",
    "        \n",
    "        for (i <- 0 until residual.length) {\n",
    "            residual (i) = kernel_norm (Y (::, i), kernel_func) - residual (i)\n",
    "        }\n",
    "        \n",
    "        residual\n",
    "    }\n",
    "\n",
    "    def kernel_gram (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        kernel_func:(breeze.linalg.DenseVector[Double], breeze.linalg.DenseVector[Double]) => Double\n",
    "    ):breeze.linalg.DenseMatrix[Double] = {\n",
    "        val gram = new breeze.linalg.DenseMatrix[Double](X.cols, X.cols)\n",
    "        var k_ij = 0.0\n",
    "        var Xi:breeze.linalg.DenseVector[Double] = null\n",
    "\n",
    "        for (i <- 0 until X.cols) {\n",
    "            Xi = X(::, i)\n",
    "\n",
    "            for (j <- 0 until i) {\n",
    "                k_ij = kernel_func (Xi, X(::, j))\n",
    "                gram(i, j) = k_ij\n",
    "                gram(j, i) = k_ij\n",
    "            }\n",
    "\n",
    "            gram (i, i) = kernel_func (Xi, Xi)\n",
    "        }\n",
    "\n",
    "        gram\n",
    "    }\n",
    "    \n",
    "    def inplace_kernel_gram (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        kernel_func:(breeze.linalg.DenseVector[Double], breeze.linalg.DenseVector[Double]) => Double,\n",
    "        out:breeze.linalg.DenseMatrix[Double]\n",
    "    ):Unit = {\n",
    "        assert (\n",
    "            X.cols == out.cols && X.cols == out.rows, \n",
    "            \"\"\"Output Matrix did not match input X dimension\n",
    "              |Output dims ${out.rows} x ${out.cols}\n",
    "              |X dim ${X.cols}\"\"\"\n",
    "        )\n",
    "        \n",
    "        var k_ij = 0.0\n",
    "        var Xi:breeze.linalg.DenseVector[Double] = null\n",
    "\n",
    "        for (i <- 0 until X.cols) {\n",
    "            Xi = X(::, i)\n",
    "\n",
    "            for (j <- 0 until i) {\n",
    "                k_ij = kernel_func (Xi, X(::, j))\n",
    "                out(i, j) = k_ij\n",
    "                out(j, i) = k_ij\n",
    "            }\n",
    "\n",
    "            out (i, i) = kernel_func (Xi, Xi)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def eval_chol (\n",
    "        X:breeze.linalg.DenseMatrix[Double], \n",
    "        regularizer_diag:breeze.linalg.DenseVector[Double], \n",
    "        lambda:Double,\n",
    "        kernel_func:(breeze.linalg.DenseVector[Double], breeze.linalg.DenseVector[Double]) => Double\n",
    "    ):breeze.linalg.DenseMatrix[Double] = {\n",
    "        val eventual_out_mat = kernel_gram (X, kernel_func)\n",
    "\n",
    "        // slightly inefficient but garantees a pure function\n",
    "        eventual_out_mat += breeze.linalg.diag (lambda * regularizer_diag)\n",
    "        \n",
    "        //cholesky\n",
    "        LinalgUtils.implace_cholesky (eventual_out_mat)\n",
    "        \n",
    "        eventual_out_mat\n",
    "    }\n",
    "    \n",
    "    def implace_eval_chol (\n",
    "        X:breeze.linalg.DenseMatrix[Double], \n",
    "        regularizer_diag:breeze.linalg.DenseVector[Double], \n",
    "        lambda:Double,\n",
    "        kernel_func:(breeze.linalg.DenseVector[Double], breeze.linalg.DenseVector[Double]) => Double,\n",
    "        eventual_out_mat:breeze.linalg.DenseMatrix[Double]\n",
    "    ):Unit = {\n",
    "        inplace_kernel_gram (X, kernel_func, eventual_out_mat)\n",
    "\n",
    "        // slightly inefficient but garantees a pure function\n",
    "        eventual_out_mat += breeze.linalg.diag (lambda * regularizer_diag)\n",
    "        \n",
    "        //cholesky\n",
    "        LinalgUtils.implace_cholesky (eventual_out_mat)  \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:27:49.246120Z",
     "start_time": "2019-04-19T21:26:35.485Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "object KernelPack {\n",
    "    def apply (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        regularizer_diag:breeze.linalg.DenseVector[Double], \n",
    "        lambda:Double,\n",
    "        kernel_func:(\n",
    "            breeze.linalg.DenseVector[Double], \n",
    "            breeze.linalg.DenseVector[Double]\n",
    "        ) => Double\n",
    "    ) = {\n",
    "        val kpack = new KernelPack (\n",
    "            kernel_func\n",
    "        )\n",
    "\n",
    "        kpack.update (X, regularizer_diag, lambda)\n",
    "\n",
    "        kpack\n",
    "    }\n",
    "\n",
    "    \n",
    "    def knorm_factory (\n",
    "        kernel_func:(\n",
    "            breeze.linalg.DenseVector[Double], \n",
    "            breeze.linalg.DenseVector[Double]\n",
    "        ) => Double\n",
    "    ) = {\n",
    "        org.apache.spark.sql.functions.udf (\n",
    "            (y:Seq[Double]) => KernelUtils.kernel_norm (\n",
    "                new breeze.linalg.DenseVector (y.toArray),\n",
    "                kernel_func\n",
    "            )\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    def kproj_factory (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        sqrt_GX:breeze.linalg.DenseMatrix[Double],\n",
    "        kernel_func:(\n",
    "            breeze.linalg.DenseVector[Double], \n",
    "            breeze.linalg.DenseVector[Double]\n",
    "        ) => Double\n",
    "    ) = {\n",
    "        org.apache.spark.sql.functions.udf (\n",
    "            (y:Seq[Double]) => KernelUtils.kernel_proj (\n",
    "                X, new breeze.linalg.DenseVector (y.toArray), sqrt_GX, kernel_func\n",
    "            ).data.toSeq\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    def kleverage_factory (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        sqrt_GX:breeze.linalg.DenseMatrix[Double],\n",
    "        kernel_func:(\n",
    "            breeze.linalg.DenseVector[Double], \n",
    "            breeze.linalg.DenseVector[Double]\n",
    "        ) => Double\n",
    "    ) = {\n",
    "        org.apache.spark.sql.functions.udf (\n",
    "            (y:Seq[Double]) => {\n",
    "                val dense_y = new breeze.linalg.DenseVector (y.toArray)\n",
    "                val y_proj = KernelUtils.kernel_proj (\n",
    "                    X, dense_y, sqrt_GX, kernel_func\n",
    "                )\n",
    "\n",
    "                KernelUtils.kernel_norm (\n",
    "                    dense_y, kernel_func\n",
    "                ) - (y_proj.t * y_proj)\n",
    "            }:Double\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "class KernelPack private (\n",
    "    val kernel_func:(\n",
    "        breeze.linalg.DenseVector[Double], \n",
    "        breeze.linalg.DenseVector[Double]\n",
    "    ) => Double\n",
    ") extends Serializable {\n",
    "    private var _kproj_udf:org.apache.spark.sql.expressions.UserDefinedFunction = null\n",
    "    private var _kleverage_udf:org.apache.spark.sql.expressions.UserDefinedFunction = null\n",
    "    \n",
    "    def kproj_udf () = _kproj_udf\n",
    "    def kleverage_udf () = _kleverage_udf\n",
    "\n",
    "    val knorm_udf = KernelPack.knorm_factory (kernel_func)\n",
    "\n",
    "    def update (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        regularizer_diag:breeze.linalg.DenseVector[Double], \n",
    "        lambda:Double\n",
    "    ):Unit = {\n",
    "        val sqrt_GX = KernelUtils.eval_chol (\n",
    "            X, regularizer_diag, \n",
    "            lambda, this.kernel_func\n",
    "        )\n",
    "        \n",
    "        this._kproj_udf = KernelPack.kproj_factory (X, sqrt_GX, this.kernel_func)\n",
    "        this._kleverage_udf = KernelPack.kleverage_factory (X, sqrt_GX, this.kernel_func)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:27:52.327359Z",
     "start_time": "2019-04-19T21:26:35.488Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "object KernelSpaces {\n",
    "    def apply (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        regularizer_diag:breeze.linalg.DenseVector[Double], \n",
    "        X_ids:Seq[Long],\n",
    "        lambda:Double,\n",
    "        kernel_func:(\n",
    "            breeze.linalg.DenseVector[Double], \n",
    "            breeze.linalg.DenseVector[Double]\n",
    "        ) => Double,\n",
    "        Xs_max_count:Int\n",
    "    ) = {\n",
    "        val kspaces = new KernelSpaces (\n",
    "            new Array[breeze.linalg.DenseMatrix[Double]] (Xs_max_count), // Xs\n",
    "            new Array[breeze.linalg.DenseVector[Double]] (Xs_max_count), // regularizer_diags\n",
    "            new Array[breeze.linalg.DenseMatrix[Double]] (Xs_max_count), // sqrt_GXs\n",
    "            new Array[Seq[Long]] (Xs_max_count), // Xs_ids\n",
    "            kernel_func\n",
    "        )\n",
    "\n",
    "        kspaces.update_lambda (lambda)\n",
    "\n",
    "        kspaces.append (\n",
    "            X,\n",
    "            regularizer_diag,\n",
    "            X_ids\n",
    "        )\n",
    "        \n",
    "        kspaces\n",
    "    }\n",
    "    \n",
    "    def knorm_factory (\n",
    "        kernel_func:(\n",
    "            breeze.linalg.DenseVector[Double], \n",
    "            breeze.linalg.DenseVector[Double]\n",
    "        ) => Double\n",
    "    ) = {\n",
    "        org.apache.spark.sql.functions.udf (\n",
    "            (y:Seq[Double]) => KernelUtils.kernel_norm (\n",
    "                new breeze.linalg.DenseVector (y.toArray),\n",
    "                kernel_func\n",
    "            )\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    def kproj_factory (\n",
    "        Xs:Seq[breeze.linalg.DenseMatrix[Double]],\n",
    "        sqrt_GXs:Seq[breeze.linalg.DenseMatrix[Double]],\n",
    "        kernel_func:(\n",
    "            breeze.linalg.DenseVector[Double], \n",
    "            breeze.linalg.DenseVector[Double]\n",
    "        ) => Double\n",
    "    ) = {\n",
    "        val proj_len = Xs.map (_.cols).sum\n",
    "        val kspace_pairs = Xs.zip (sqrt_GXs)\n",
    "\n",
    "        org.apache.spark.sql.functions.udf (\n",
    "            (y:Seq[Double]) => {\n",
    "                val dense_y = new breeze.linalg.DenseVector (y.toArray)\n",
    "                val y_proj = new breeze.linalg.DenseVector[Double] (proj_len) \n",
    "\n",
    "                var offset:Int = 0\n",
    "\n",
    "                // aparently pattern matching requires lowercase...\n",
    "                for ((aX, sqrt_GX) <- kspace_pairs) {\n",
    "                    KernelUtils.buff_kernel_proj (\n",
    "                        aX, dense_y, sqrt_GX, kernel_func,\n",
    "                        y_proj(offset until offset + aX.cols)\n",
    "                    )\n",
    "                }\n",
    "\n",
    "                y_proj.data.toSeq\n",
    "            }\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    // leverage is evaluated via \n",
    "    // knorm - sum (leverages w.r.t. each X)\n",
    "    // may turn out negative\n",
    "    // there may be alternatives\n",
    "    def pseudo_kleverage_factory (\n",
    "        Xs:Seq[breeze.linalg.DenseMatrix[Double]],\n",
    "        sqrt_GXs:Seq[breeze.linalg.DenseMatrix[Double]],\n",
    "        kernel_func:(\n",
    "            breeze.linalg.DenseVector[Double], \n",
    "            breeze.linalg.DenseVector[Double]\n",
    "        ) => Double\n",
    "    ) = {\n",
    "        val buffer_len = Xs.map (_.cols).max\n",
    "        val kspace_pairs = Xs.zip (sqrt_GXs)\n",
    "\n",
    "        org.apache.spark.sql.functions.udf (\n",
    "            (y:Seq[Double]) => {\n",
    "                val dense_y = new breeze.linalg.DenseVector (y.toArray)\n",
    "                val buff_y_proj = new breeze.linalg.DenseVector[Double] (buffer_len)\n",
    "\n",
    "                var residual = KernelUtils.kernel_norm (\n",
    "                    dense_y, kernel_func\n",
    "                )\n",
    "\n",
    "                // aparently pattern matching requires lowercase...\n",
    "                for ((aX, sqrt_GX) <- kspace_pairs) {\n",
    "                    KernelUtils.buff_kernel_proj (\n",
    "                        aX, dense_y, sqrt_GX, kernel_func,\n",
    "                        buff_y_proj(0 until aX.cols)\n",
    "                    )\n",
    "                    residual -= buff_y_proj(0 until aX.cols).t * buff_y_proj(0 until aX.cols)\n",
    "                }\n",
    "\n",
    "                residual\n",
    "            }:Double\n",
    "        )\n",
    "    }\n",
    "\n",
    "    def min_kleverage_factory (\n",
    "        Xs:Seq[breeze.linalg.DenseMatrix[Double]],\n",
    "        sqrt_GXs:Seq[breeze.linalg.DenseMatrix[Double]],\n",
    "        kernel_func:(\n",
    "            breeze.linalg.DenseVector[Double], \n",
    "            breeze.linalg.DenseVector[Double]\n",
    "        ) => Double\n",
    "    ) = {\n",
    "        val buffer_len = Xs.map (_.cols).max\n",
    "        val kspace_pairs = Xs.zip (sqrt_GXs)\n",
    "\n",
    "        org.apache.spark.sql.functions.udf (\n",
    "            (y:Seq[Double]) => {\n",
    "                val dense_y = new breeze.linalg.DenseVector (y.toArray)\n",
    "                val buff_y_proj = new breeze.linalg.DenseVector[Double] (buffer_len)\n",
    "\n",
    "                val norm = KernelUtils.kernel_norm (\n",
    "                    dense_y, kernel_func\n",
    "                )\n",
    "\n",
    "                // aparently pattern matching requires lowercase...\n",
    "                kspace_pairs.map {\n",
    "                    case (aX, sqrt_GX) => {\n",
    "                        KernelUtils.buff_kernel_proj (\n",
    "                            aX, dense_y, sqrt_GX, kernel_func,\n",
    "                            buff_y_proj(0 until aX.cols)\n",
    "                        )\n",
    "                        norm - buff_y_proj(0 until aX.cols).t * buff_y_proj(0 until aX.cols)\n",
    "                    }\n",
    "                }.min\n",
    "            }:Double\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "class KernelSpaces private (\n",
    "    private val _Xs:Array[breeze.linalg.DenseMatrix[Double]],\n",
    "    private val _regularizer_diags:Array[breeze.linalg.DenseVector[Double]],\n",
    "    private val _sqrt_GXs:Array[breeze.linalg.DenseMatrix[Double]],\n",
    "    private val _Xs_ids:Array[Seq[Long]],\n",
    "    val kernel_func:(\n",
    "        breeze.linalg.DenseVector[Double], \n",
    "        breeze.linalg.DenseVector[Double]\n",
    "    ) => Double\n",
    ") extends Serializable {\n",
    "    private var _kproj_udf:org.apache.spark.sql.expressions.UserDefinedFunction = null\n",
    "    private var _pseudo_kleverage_udf:org.apache.spark.sql.expressions.UserDefinedFunction = null\n",
    "    private var _min_kleverage_udf:org.apache.spark.sql.expressions.UserDefinedFunction = null\n",
    "    \n",
    "    private var _Xs_count:Int = 0\n",
    "    private var _Xs_last_id:Int = 0\n",
    "    \n",
    "    // can be updated, updating all subspaces\n",
    "    private var _lambda:Double = 0\n",
    "\n",
    "    val Xs_length = this._Xs.length\n",
    "\n",
    "    def kproj_udf () = this._kproj_udf\n",
    "    def pseudo_kleverage_udf () = this._pseudo_kleverage_udf\n",
    "    def min_kleverage_udf () = this._min_kleverage_udf\n",
    "\n",
    "    def lambda () = this._lambda\n",
    "    def Xs_count () = this._Xs_count\n",
    "    def Xs_last_id () = this._Xs_last_id\n",
    "\n",
    "    def Xs () = this._Xs.slice (0, this._Xs_count).toSeq\n",
    "    def sqrt_GXs () = this._sqrt_GXs.slice (0, this._Xs_count).toSeq\n",
    "    def regularizer_diags () = this._regularizer_diags.slice (0, this._Xs_count).toSeq\n",
    "    def Xs_ids () = this._Xs_ids.slice (0, this._Xs_count).toSeq\n",
    "\n",
    "    private def update_udfs ():Unit = {\n",
    "        // reconstructs udfs\n",
    "        this._kproj_udf = KernelSpaces.kproj_factory (\n",
    "            this._Xs.slice (0, this._Xs_count).toSeq, \n",
    "            this._sqrt_GXs.slice (0, this._Xs_count).toSeq, \n",
    "            this.kernel_func\n",
    "        )\n",
    "        \n",
    "        this._pseudo_kleverage_udf = KernelSpaces.pseudo_kleverage_factory (\n",
    "            this._Xs.slice (0, this._Xs_count).toSeq, \n",
    "            this._sqrt_GXs.slice (0, this._Xs_count).toSeq, \n",
    "            this.kernel_func\n",
    "        )\n",
    "\n",
    "        this._min_kleverage_udf = KernelSpaces.min_kleverage_factory (\n",
    "            this._Xs.slice (0, this._Xs_count).toSeq, \n",
    "            this._sqrt_GXs.slice (0, this._Xs_count).toSeq, \n",
    "            this.kernel_func\n",
    "        )\n",
    "    }\n",
    "\n",
    "    def append (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        regularizer_diag:breeze.linalg.DenseVector[Double], \n",
    "        X_ids:Seq[Long]\n",
    "    ):Unit = {\n",
    "        assert (this.Xs_length > this._Xs_count, \"Cannot append anymore matrices, Array full\")\n",
    "\n",
    "        // adding new matrices and vectors\n",
    "        this._Xs (this._Xs_count) = X\n",
    "        this._regularizer_diags (this._Xs_count) = regularizer_diag\n",
    "        this._Xs_ids (this._Xs_count) = X_ids\n",
    "        \n",
    "        this._sqrt_GXs (this._Xs_count) = KernelUtils.eval_chol (\n",
    "            X, regularizer_diag, \n",
    "            this._lambda, this.kernel_func\n",
    "        )\n",
    "\n",
    "        // updates count\n",
    "        this._Xs_count += 1\n",
    "        this._Xs_last_id += 1\n",
    "\n",
    "        this.update_udfs\n",
    "    }\n",
    "\n",
    "    // overwrites matrices when comming back around the ids\n",
    "    def append_circular (\n",
    "        X:breeze.linalg.DenseMatrix[Double],\n",
    "        regularizer_diag:breeze.linalg.DenseVector[Double], \n",
    "        X_ids:Seq[Long]        \n",
    "    ):Unit = {\n",
    "        // caps out last id if necessary\n",
    "        // helps with not needing to do it anywhere else\n",
    "        if (this._Xs_last_id >= this.Xs_length) {\n",
    "            this._Xs_last_id = 0\n",
    "        }\n",
    "        \n",
    "        // adding new matrices and vectors\n",
    "        this._Xs (this._Xs_last_id) = X\n",
    "        this._regularizer_diags (this._Xs_last_id) = regularizer_diag\n",
    "        this._Xs_ids (this._Xs_count) = X_ids\n",
    "        \n",
    "        this._sqrt_GXs (this._Xs_last_id) = KernelUtils.eval_chol (\n",
    "            X, regularizer_diag, \n",
    "            this._lambda, this.kernel_func\n",
    "        )\n",
    "\n",
    "        // updates count and last id\n",
    "        this._Xs_last_id += 1\n",
    "        \n",
    "        if (this._Xs_count < this.Xs_length) {\n",
    "            this._Xs_count += 1\n",
    "        }\n",
    "\n",
    "        this.update_udfs\n",
    "    }\n",
    "    \n",
    "    def remove_id (i:Int) {\n",
    "        assert (i < this._Xs_count && i >= 0, f\"Index ${i} out of interval [0, ${this._Xs_count})\")\n",
    "        \n",
    "        // sets entries to null, because gc (?)\n",
    "        this._Xs (i) = null\n",
    "        this._sqrt_GXs (i) = null\n",
    "        this._regularizer_diags (i) = null\n",
    "        this._Xs_ids (i) = null\n",
    "        \n",
    "        // decrements length\n",
    "        this._Xs_count -= 1\n",
    "        \n",
    "        // sets i to the highest index's (this._Xs_count) values\n",
    "        this._Xs (i) = this._Xs (this._Xs_count)\n",
    "        this._sqrt_GXs (i) = this._sqrt_GXs (this._Xs_count)\n",
    "        this._regularizer_diags (i) = this._regularizer_diags (this._Xs_count)\n",
    "        this._Xs_ids (i) = this._Xs_ids (this._Xs_count)\n",
    "        \n",
    "        this.update_udfs\n",
    "    }\n",
    "\n",
    "    // updates all subspaces to conform to new regularization\n",
    "    // up to understanding how to update without reevaluating cholesky factors\n",
    "    def update_lambda (\n",
    "        lambda:Double\n",
    "    ):Unit = {\n",
    "        this._lambda = lambda\n",
    "\n",
    "        // less memory leaky\n",
    "        for (\n",
    "            ((aX, sqrt_GX), regularizer_diag) <- this._Xs.slice (\n",
    "                0, this._Xs_count\n",
    "            ).zip (\n",
    "                this._sqrt_GXs\n",
    "            ).zip (this._regularizer_diags)\n",
    "        ) {\n",
    "            KernelUtils.implace_eval_chol (\n",
    "                aX, regularizer_diag, \n",
    "                this._lambda, this.kernel_func,\n",
    "                sqrt_GX\n",
    "            )\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## kernel func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:27:53.254649Z",
     "start_time": "2019-04-19T21:26:35.492Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def kernel_func (\n",
    "    x:breeze.linalg.DenseVector[Double], \n",
    "    y:breeze.linalg.DenseVector[Double]\n",
    "):Double = x.t * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Spark Digits Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:27:53.519764Z",
     "start_time": "2019-04-19T21:26:35.495Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.{sql => s_sql}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:27:54.541337Z",
     "start_time": "2019-04-19T21:26:35.497Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val max_per_P = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:28:22.059708Z",
     "start_time": "2019-04-19T21:28:19.975Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val path = \"hdfs://thesis-tiny-python3-anaconda-m/user/pedro.schmidt/digits.snappy.parquet\"\n",
    "//val path = \"digits.snappy.parquet\"\n",
    "\n",
    "var df_digits_raw = spark.read.format (\n",
    "    \"parquet\"\n",
    ").load (path).withColumn (\n",
    "    \"features\", $\"features\".cast (\"ARRAY<DOUBLE>\")\n",
    ").withColumn (\n",
    "    \"id\", s_sql.functions.monotonically_increasing_id\n",
    ")\n",
    "\n",
    "df_digits_raw = df_digits_raw.repartition (df_digits_raw.count ().toInt / max_per_P).persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:27:58.454485Z",
     "start_time": "2019-04-19T21:26:35.502Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_digits_raw.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nystrom algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Features transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:27:58.855087Z",
     "start_time": "2019-04-19T21:26:35.504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val image_dim = scala.math.sqrt (\n",
    "    df_digits_raw.select (\n",
    "        s_sql.functions.size ($\"features\")\n",
    "    ).map (_.getInt (0)).take (1)(0)\n",
    ").toInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:27:59.622133Z",
     "start_time": "2019-04-19T21:26:35.506Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// window is centered\n",
    "\n",
    "val window_x = 7\n",
    "val window_y = 7\n",
    "val skip_x = 3\n",
    "val skip_y = 3\n",
    "\n",
    "def max_pool (\n",
    "    I:breeze.linalg.DenseMatrix[Double]\n",
    "):breeze.linalg.DenseMatrix[Double] = {\n",
    "    // column major implies image will be\n",
    "    val I_pooled = new breeze.linalg.DenseMatrix[Double](\n",
    "        (I.rows - window_y) / skip_y, \n",
    "        (I.cols - window_x) / skip_x\n",
    "    )\n",
    "    \n",
    "    for (\n",
    "        (i, im, ip) <- (0 until I_pooled.rows).map (\n",
    "            (v:Int) => (v, skip_y * v, skip_y * v + window_y)\n",
    "        );\n",
    "        (j, jm, jp) <- (0 until I_pooled.cols).map (\n",
    "            (v:Int) => (v, skip_x * v, skip_x * v + window_x)\n",
    "        )\n",
    "    ) {\n",
    "        I_pooled (i, j) = breeze.linalg.max (I (im until ip, jm until jp))\n",
    "    }\n",
    "    \n",
    "    I_pooled\n",
    "}\n",
    "\n",
    "val max_pool_udf = s_sql.functions.udf (\n",
    "    (x:Seq[Double]) => {\n",
    "        max_pool (\n",
    "            new breeze.linalg.DenseMatrix (image_dim, image_dim, x.toArray)\n",
    "        ).data.toSeq\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:28:00.009154Z",
     "start_time": "2019-04-19T21:26:35.508Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val df_data = df_digits_raw.select (\n",
    "    LinalgUtils.normalize_udf (\n",
    "        $\"features\"\n",
    "//         max_pool_udf ($\"features\")\n",
    "    ).as (\"nfeatures\"), $\"label\", $\"id\"\n",
    ").persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:28:00.344688Z",
     "start_time": "2019-04-19T21:26:35.509Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val n = df_data.count ().toInt\n",
    "val rows = df_data.select (\n",
    "    s_sql.functions.size ($\"nfeatures\")\n",
    ").map (_.getInt (0)).take (1)(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Nystrom Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:49:35.356678Z",
     "start_time": "2019-04-10T18:49:11.795Z"
    },
    "hidden": true
   },
   "source": [
    "val kappa = 1.0\n",
    "val t = .3\n",
    "val base_lamb = kappa * kappa / scala.math.min (t, 1.0)\n",
    "val q = 1.5\n",
    "// target lambda is 1 / sqrt (n)\n",
    "val H = (scala.math.log (base_lamb * scala.math.sqrt (n)) / scala.math.log (q)).toInt\n",
    "val delta = .9\n",
    "\n",
    "val q2 = 54 * (kappa * kappa) * ((2 + t * t) / (t * t)) * scala.math.log  (12 * H * n / delta)\n",
    "\n",
    "val max_rows = 5000\n",
    "\n",
    "val base_beta = scala.math.min (q2 * kappa * kappa / (base_lamb * n), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:49:35.947644Z",
     "start_time": "2019-04-10T18:49:11.796Z"
    },
    "hidden": true
   },
   "source": [
    "var lambda_h = base_lamb\n",
    "var beta_h = base_beta\n",
    "\n",
    "//step 0, i.e. initialization step\n",
    "var df_sample_set_h:org.apache.spark.sql.Dataset[\n",
    "    org.apache.spark.sql.Row\n",
    "] = df_data.select (\"nfeatures\").sample (beta_h).withColumn (\n",
    "    \"ph_cap\",\n",
    "    LinalgUtils.cap_to_one_udf (\n",
    "        // first round does not have a kpack\n",
    "        // with most kernels, kinda meaningless...\n",
    "        KernelPack.knorm_factory (kernel_func (_,_))($\"nfeatures\").multiply (q2)\n",
    "    )\n",
    ").filter (\n",
    "    s_sql.functions.rand ().multiply (beta_h) <= $\"ph_cap\"\n",
    ").orderBy (\"ph_cap\").limit (max_rows)\n",
    "\n",
    "// sample length\n",
    "// runs most of the computation, lol\n",
    "var cols = df_sample_set_h.count ().toInt\n",
    "\n",
    "val kpack_h = KernelPack (\n",
    "    // X\n",
    "    new breeze.linalg.DenseMatrix (\n",
    "        rows, cols, \n",
    "        df_sample_set_h.select (\"nfeatures\").flatMap {\n",
    "            case row: s_sql.Row => row.getSeq[Double](0)\n",
    "        }.collect ().toArray\n",
    "    ), \n",
    "    // A\n",
    "    new breeze.linalg.DenseVector (\n",
    "        df_sample_set_h.select (\"ph_cap\").map {\n",
    "            case row: s_sql.Row => row.getDouble(0)\n",
    "        }.collect ().toArray\n",
    "    ), \n",
    "    // lambda * n\n",
    "    lambda_h * n, \n",
    "    kernel_func (_,_)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T18:49:36.502641Z",
     "start_time": "2019-04-10T18:49:11.798Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "source": [
    "lambda_h = base_lamb\n",
    "beta_h = base_beta\n",
    "\n",
    "for (i <- 1 until H) {\n",
    "    println (\"----------------------------------------------\")\n",
    "    println (f\"Step: ${i}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    // step i\n",
    "    // lambda and beta updates\n",
    "    lambda_h /= q\n",
    "    beta_h = scala.math.min (q2 * kappa * kappa / (lambda_h * n), 1.0)\n",
    "    \n",
    "    println (f\"lambda_h: ${lambda_h}, beta_h: ${beta_h}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    // new sample\n",
    "    df_sample_set_h = df_data.select (\"nfeatures\").sample (beta_h).withColumn (\n",
    "        \"ph_cap\",\n",
    "        LinalgUtils.cap_to_one_udf (kpack_h.kleverage_udf ($\"nfeatures\").multiply (q2))\n",
    "    ).filter (\n",
    "        s_sql.functions.rand ().multiply (beta_h) <= $\"ph_cap\"\n",
    "    ).orderBy (s_sql.functions.negate ($\"ph_cap\")).limit (max_rows)\n",
    "\n",
    "    // sample length\n",
    "    // runs most of the computation, lol\n",
    "    cols = df_sample_set_h.count ().toInt\n",
    "\n",
    "    // kernel udfs update\n",
    "    kpack_h.update (\n",
    "        // X\n",
    "        new breeze.linalg.DenseMatrix (\n",
    "            rows, cols, \n",
    "            df_sample_set_h.select (\"nfeatures\").flatMap {\n",
    "                case row: s_sql.Row => row.getSeq[Double](0)\n",
    "            }.collect ().toArray\n",
    "        ), \n",
    "        // A\n",
    "        new breeze.linalg.DenseVector (\n",
    "            df_sample_set_h.select (\"ph_cap\").map {\n",
    "                case row: s_sql.Row => row.getDouble(0)\n",
    "            }.collect ().toArray\n",
    "        ), \n",
    "        // lambda * n\n",
    "        lambda_h * n\n",
    "    )\n",
    "    \n",
    "    println (f\"cols: ${cols}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    // global leverage score\n",
    "    df_data.select (\n",
    "        kpack_h.kleverage_udf ($\"nfeatures\").as (\"ph\")\n",
    "    ).agg (s_sql.functions.sum ($\"ph\")).show\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nystrom spaces loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T20:31:40.168718Z",
     "start_time": "2019-04-19T20:31:36.413Z"
    }
   },
   "outputs": [],
   "source": [
    "val kappa = 1.0\n",
    "val t = .3\n",
    "val base_lamb = kappa * kappa / scala.math.min (t, 1.0)\n",
    "val q = 2\n",
    "// target lambda is 1 / sqrt (n)\n",
    "val H = (scala.math.log (base_lamb * scala.math.sqrt (n)) / scala.math.log (q)).toInt\n",
    "val delta = .9\n",
    "\n",
    "// val q2 = 54 * (kappa * kappa) * ((2 + t * t) / (t * t)) * scala.math.log  (12 * H * n / delta)\n",
    "val q2 = (kappa * kappa) * ((2 + t * t) / (t * t)) * scala.math.log  (12 * H * n / delta)\n",
    "\n",
    "val max_rows = 1000\n",
    "val max_spaces = 4\n",
    "val max_non_improv = 10\n",
    "\n",
    "val base_beta = scala.math.min (q2 * kappa * kappa / (base_lamb), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T20:46:13.831730Z",
     "start_time": "2019-04-19T20:45:25.815Z"
    }
   },
   "outputs": [],
   "source": [
    "var lambda_h = base_lamb\n",
    "var beta_h = base_beta\n",
    "\n",
    "//step 0, i.e. initialization step\n",
    "var df_sample_set_h:org.apache.spark.sql.Dataset[\n",
    "    org.apache.spark.sql.Row\n",
    "] = df_data.select ($\"nfeatures\", $\"id\").sample (beta_h).withColumn (\n",
    "    \"ph_cap\",\n",
    "    LinalgUtils.cap_to_one_udf (\n",
    "        // first round does not have a kpack\n",
    "        // with most kernels, kinda meaningless...\n",
    "        KernelPack.knorm_factory (kernel_func (_,_))($\"nfeatures\").multiply (scala.math.log  (12 * H * n / delta))\n",
    "    )\n",
    ").filter (\n",
    "    s_sql.functions.rand () <= LinalgUtils.cap_to_one_udf ($\"ph_cap\")\n",
    ").orderBy (s_sql.functions.negate ($\"ph_cap\")).limit (max_rows).persist\n",
    "\n",
    "// sample length\n",
    "var cols = df_sample_set_h.count ().toInt\n",
    "\n",
    "val kspaces_h = KernelSpaces (\n",
    "    // X\n",
    "    new breeze.linalg.DenseMatrix (\n",
    "        rows, cols, \n",
    "        df_sample_set_h.select (\"nfeatures\").flatMap {\n",
    "            case row: s_sql.Row => row.getSeq[Double](0)\n",
    "        }.collect ().toArray\n",
    "    ), \n",
    "    // A\n",
    "    new breeze.linalg.DenseVector (\n",
    "        df_sample_set_h.select (\n",
    "            LinalgUtils.cap_to_one_udf ($\"ph_cap\")\n",
    "        ).map {\n",
    "            case row: s_sql.Row => row.getDouble(0)\n",
    "        }.collect ().toArray\n",
    "    ),\n",
    "    // X_ids\n",
    "    df_sample_set_h.select (\n",
    "        $\"id\"\n",
    "    ).map {\n",
    "        case row: s_sql.Row => row.getLong(0)\n",
    "    }.collect (),\n",
    "    // lambda * n\n",
    "    lambda_h,// * n, \n",
    "    kernel_func (_,_),\n",
    "    max_spaces\n",
    ")\n",
    "\n",
    "df_sample_set_h = df_sample_set_h.unpersist (true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var non_improv_count = 0\n",
    "var min_leverage:Double = df_data.agg (\n",
    "    s_sql.functions.sum (kspaces_h.min_kleverage_udf () ($\"nfeatures\").as (\"best_ph\"))\n",
    ").map {\n",
    "    case row: s_sql.Row => row.getDouble(0)\n",
    "}.collect ()(0)\n",
    "\n",
    "var current_leverage:Double = 0.0\n",
    "\n",
    "var best_Xs = kspaces_h.Xs\n",
    "var best_sqrt_GXs = kspaces_h.sqrt_GXs\n",
    "var best_regularizer_diags = kspaces_h.regularizer_diags\n",
    "var best_ids = kspaces_h.Xs_ids\n",
    "\n",
    "var i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T19:07:25.151850Z",
     "start_time": "2019-04-16T18:57:01.605Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "lambda_h = base_lamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retain best current set of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T19:07:25.151850Z",
     "start_time": "2019-04-16T18:57:01.605Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while (non_improv_count < max_non_improv) {\n",
    "    println (\"----------------------------------------------\")\n",
    "    println (f\"i: ${i}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    // step i\n",
    "    // lambda and beta updates\n",
    "    \n",
    "    if (i < H) {\n",
    "        lambda_h /= q\n",
    "    }\n",
    "\n",
    "    beta_h = scala.math.min (q2 * kappa * kappa / (lambda_h), 1.0)\n",
    "    \n",
    "    println (f\"lambda_h: ${lambda_h}, beta_h: ${beta_h}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    // removal of subspaces when array gets full\n",
    "    if (kspaces_h.Xs_length <= i) {\n",
    "        println (f\"removed a X at i: ${(i - 1) % (kspaces_h.Xs_length - 1)}\")\n",
    "        println (\"----------------------------------------------\")\n",
    "        kspaces_h.remove_id ((i - 1) % (kspaces_h.Xs_length - 1))\n",
    "    }\n",
    "    \n",
    "    kspaces_h.update_lambda (\n",
    "        // lambda * n\n",
    "        lambda_h// * n\n",
    "    )\n",
    "    \n",
    "    // new sample\n",
    "    df_sample_set_h = df_data.select ($\"nfeatures\", $\"id\").sample (beta_h).withColumn (\n",
    "        \"ph_cap\",\n",
    "        kspaces_h.min_kleverage_udf () ($\"nfeatures\").multiply (scala.math.log  (12 * H * n / delta))\n",
    "    ).filter (\n",
    "        s_sql.functions.rand () <= LinalgUtils.cap_to_one_udf ($\"ph_cap\")\n",
    "    ).orderBy (s_sql.functions.negate ($\"ph_cap\")).limit (max_rows).persist\n",
    "\n",
    "    // sample length\n",
    "    cols = df_sample_set_h.count ().toInt\n",
    "\n",
    "    // kernel udfs update\n",
    "    kspaces_h.append (\n",
    "        // X\n",
    "        new breeze.linalg.DenseMatrix (\n",
    "            rows, cols, \n",
    "            df_sample_set_h.select (\"nfeatures\").flatMap {\n",
    "                case row: s_sql.Row => row.getSeq[Double](0)\n",
    "            }.collect ().toArray\n",
    "        ), \n",
    "        // A\n",
    "        new breeze.linalg.DenseVector (\n",
    "            df_sample_set_h.select (\n",
    "                LinalgUtils.cap_to_one_udf ($\"ph_cap\")\n",
    "            ).map {\n",
    "                case row: s_sql.Row => row.getDouble(0)\n",
    "            }.collect ().toArray\n",
    "        ),\n",
    "        // X_ids\n",
    "        df_sample_set_h.select (\n",
    "            $\"id\"\n",
    "        ).map {\n",
    "            case row: s_sql.Row => row.getLong(0)\n",
    "        }.collect ()\n",
    "    )\n",
    "    \n",
    "    println (f\"cols: ${cols}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    df_sample_set_h = df_sample_set_h.unpersist (true)\n",
    "    \n",
    "    println (\"Cross leverages between the Xis\")\n",
    "    for (\n",
    "        k <- 0 until kspaces_h.Xs_count\n",
    "    ) {\n",
    "        print (k)\n",
    "        print (\"    \")\n",
    "\n",
    "        for (l <- 0 until kspaces_h.Xs_count) {\n",
    "            print (f\"\"\"${\n",
    "                breeze.linalg.sum (KernelUtils.kernel_leverages (\n",
    "                    kspaces_h.Xs ()(k),\n",
    "                    kspaces_h.Xs ()(l),\n",
    "                    kspaces_h.sqrt_GXs ()(k),\n",
    "                    kspaces_h.kernel_func\n",
    "                ))\n",
    "            }%.2f\"\"\")\n",
    "            print (\" | \")\n",
    "        }\n",
    "\n",
    "        print (\"\\n\")\n",
    "    }\n",
    "    \n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    current_leverage = df_data.agg (\n",
    "        s_sql.functions.sum (kspaces_h.min_kleverage_udf () ($\"nfeatures\").as (\"best_ph\"))\n",
    "    ).map {\n",
    "        case row: s_sql.Row => row.getDouble(0)\n",
    "    }.collect ()(0)\n",
    "    \n",
    "    println (f\"current leverage sum: ${current_leverage}, current best ${min_leverage}\")\n",
    "    println (\"----------------------------------------------\")\n",
    "    \n",
    "    if (current_leverage < min_leverage) {\n",
    "        println (f\"improvement happend!\")\n",
    "        println (\"----------------------------------------------\")\n",
    "        min_leverage = current_leverage\n",
    "        // makes so that it stops when \n",
    "        // no improvement happens\n",
    "        // max_non_improv times\n",
    "        non_improv_count -= 1\n",
    "        best_Xs = kspaces_h.Xs\n",
    "        best_sqrt_GXs = kspaces_h.sqrt_GXs\n",
    "        best_regularizer_diags = kspaces_h.regularizer_diags\n",
    "        best_ids = kspaces_h.Xs_ids\n",
    "    }\n",
    "    else {\n",
    "        non_improv_count += 1\n",
    "        println (f\"current non-improvement: ${non_improv_count}\")\n",
    "        println (\"----------------------------------------------\")\n",
    "    }\n",
    "    \n",
    "    i += 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make evalutaion for best spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T19:54:53.062053Z",
     "start_time": "2019-04-16T19:54:52.357Z"
    }
   },
   "outputs": [],
   "source": [
    "best_Xs.foreach {\n",
    "    case aX => println (aX.cols)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T19:54:57.382209Z",
     "start_time": "2019-04-16T19:54:54.924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53 | 3.06 | 0.66 | \n"
     ]
    }
   ],
   "source": [
    "for (i <- 0 until best_Xs.length) {\n",
    "    print (i)\n",
    "    print (\"    \")\n",
    "\n",
    "    for (j <- 0 until best_Xs.length) {\n",
    "        print (f\"\"\"${\n",
    "            breeze.linalg.sum (KernelUtils.kernel_leverages (\n",
    "                best_Xs (i),\n",
    "                best_Xs (j),\n",
    "                best_sqrt_GXs (i),\n",
    "                kspaces_h.kernel_func\n",
    "            ))\n",
    "        }%.2f\"\"\")\n",
    "        print (\" | \")\n",
    "    }\n",
    "\n",
    "    print (\"\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate leverage scores w.r.t each subspace\n",
    "\n",
    "evaluate 'linear independence' between the subspaces"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
